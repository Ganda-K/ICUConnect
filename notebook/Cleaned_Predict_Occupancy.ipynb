{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pediatric ICU BED DEMAND(OCCUPANCY) FORECASTING\n",
    "\n",
    "**KEY POINTS ON OUR PROBLEM**\n",
    "\n",
    "1. **Inputs vs. Outputs:** *What are the inputs and outputs for a forecast?*\n",
    "\n",
    "```\n",
    "- Inputs: Historical ICU occupancy\n",
    "- OutputL Predicted ICU Occupancy.\n",
    "```\n",
    "\n",
    "2. **Endogenous vs. Exogenous:** *What are the endogenous and exogenous variables?*\n",
    "\n",
    "```\n",
    "- The variables are endogenous, the past occupancy is used to forecast future occupancy.\n",
    "```\n",
    "\n",
    "3. **Unstructured vs. Structured:** *Are the time series variables unstructured or structured?*\n",
    "\n",
    "```\n",
    "- The variables are structured; numeric and tabular\n",
    "```\n",
    "\n",
    "4. **Regression vs. Classification:** *Are you working on a regression or classification predictive modeling* *problem? What are some alternate ways to frame your time series forecasting problem?*\n",
    "\n",
    "```\n",
    "- This is a regression modeling problem, predicting numerical values(no of ICU beds in next week)\n",
    "```\n",
    "\n",
    "5. **Univariate vs. Multivariate:**  *Are you working on a univariate or multivariate time series problem?*\n",
    "\n",
    "```\n",
    "- It is a univariate problem; forecasting only on occupied beds.\n",
    "```\n",
    "\n",
    "6. **Single-step vs. Multi-step:** *Do you require a single-step or a multi-step forecast?*\n",
    "\n",
    "```\n",
    "- This is a single-step forecast, predicting only the next 1 week of ICU occupancy.\n",
    "```\n",
    "\n",
    "7. **Static vs. Dynamic:** *Do you require a static or a dynamically updated model?*\n",
    "\n",
    "```\n",
    "- Curently our model is Static but future improvements involve dynamic so it can retrain using new admissions/discharges.\n",
    "```\n",
    "\n",
    "8. **Contiguous vs. Discontiguous:** *Are your observations contiguous or discontiguous?*\n",
    "\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pediatric ICU BED DEMAND(OCCUPANCY) FORECASTING\n",
    "\n",
    "**KEY POINTS ON OUR PROBLEM**\n",
    "\n",
    "1. **Inputs vs. Outputs:** *What are the inputs and outputs for a forecast?*\n",
    "\n",
    "```\n",
    "- Inputs: Historical ICU occupancy\n",
    "- OutputL Predicted ICU Occupancy.\n",
    "```\n",
    "\n",
    "2. **Endogenous vs. Exogenous:** *What are the endogenous and exogenous variables?*\n",
    "\n",
    "```\n",
    "- The variables are endogenous, the past occupancy is used to forecast future occupancy.\n",
    "```\n",
    "\n",
    "3. **Unstructured vs. Structured:** *Are the time series variables unstructured or structured?*\n",
    "\n",
    "```\n",
    "- The variables are structured; numeric and tabular\n",
    "```\n",
    "\n",
    "4. **Regression vs. Classification:** *Are you working on a regression or classification predictive modeling* *problem? What are some alternate ways to frame your time series forecasting problem?*\n",
    "\n",
    "```\n",
    "- This is a regression modeling problem, predicting numerical values(no of ICU beds in next week)\n",
    "```\n",
    "\n",
    "5. **Univariate vs. Multivariate:**  *Are you working on a univariate or multivariate time series problem?*\n",
    "\n",
    "```\n",
    "- It is a univariate problem; forecasting only on occupied beds.\n",
    "```\n",
    "\n",
    "6. **Single-step vs. Multi-step:** *Do you require a single-step or a multi-step forecast?*\n",
    "\n",
    "```\n",
    "- This is a single-step forecast, predicting only the next 1 week of ICU occupancy.\n",
    "```\n",
    "\n",
    "7. **Static vs. Dynamic:** *Do you require a static or a dynamically updated model?*\n",
    "\n",
    "```\n",
    "- Curently our model is Static but future improvements involve dynamic so it can retrain using new admissions/discharges.\n",
    "```\n",
    "\n",
    "8. **Contiguous vs. Discontiguous:** *Are your observations contiguous or discontiguous?*\n",
    "\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pediatric ICU BED DEMAND(OCCUPANCY) FORECASTING\n",
    "\n",
    "**KEY POINTS ON OUR PROBLEM**\n",
    "\n",
    "1. **Inputs vs. Outputs:** *What are the inputs and outputs for a forecast?*\n",
    "\n",
    "```\n",
    "- Inputs: Historical ICU occupancy\n",
    "- OutputL Predicted ICU Occupancy.\n",
    "```\n",
    "\n",
    "2. **Endogenous vs. Exogenous:** *What are the endogenous and exogenous variables?*\n",
    "\n",
    "```\n",
    "- The variables are endogenous, the past occupancy is used to forecast future occupancy.\n",
    "```\n",
    "\n",
    "3. **Unstructured vs. Structured:** *Are the time series variables unstructured or structured?*\n",
    "\n",
    "```\n",
    "- The variables are structured; numeric and tabular\n",
    "```\n",
    "\n",
    "4. **Regression vs. Classification:** *Are you working on a regression or classification predictive modeling* *problem? What are some alternate ways to frame your time series forecasting problem?*\n",
    "\n",
    "```\n",
    "- This is a regression modeling problem, predicting numerical values(no of ICU beds in next week)\n",
    "```\n",
    "\n",
    "5. **Univariate vs. Multivariate:**  *Are you working on a univariate or multivariate time series problem?*\n",
    "\n",
    "```\n",
    "- It is a univariate problem; forecasting only on occupied beds.\n",
    "```\n",
    "\n",
    "6. **Single-step vs. Multi-step:** *Do you require a single-step or a multi-step forecast?*\n",
    "\n",
    "```\n",
    "- This is a single-step forecast, predicting only the next 1 week of ICU occupancy.\n",
    "```\n",
    "\n",
    "7. **Static vs. Dynamic:** *Do you require a static or a dynamically updated model?*\n",
    "\n",
    "```\n",
    "- Curently our model is Static but future improvements involve dynamic so it can retrain using new admissions/discharges.\n",
    "```\n",
    "\n",
    "8. **Contiguous vs. Discontiguous:** *Are your observations contiguous or discontiguous?*\n",
    "\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pediatric ICU BED DEMAND(OCCUPANCY) FORECASTING\n",
    "\n",
    "**KEY POINTS ON OUR PROBLEM**\n",
    "\n",
    "1. **Inputs vs. Outputs:** *What are the inputs and outputs for a forecast?*\n",
    "\n",
    "```\n",
    "- Inputs: Historical ICU occupancy\n",
    "- OutputL Predicted ICU Occupancy.\n",
    "```\n",
    "\n",
    "2. **Endogenous vs. Exogenous:** *What are the endogenous and exogenous variables?*\n",
    "\n",
    "```\n",
    "- The variables are endogenous, the past occupancy is used to forecast future occupancy.\n",
    "```\n",
    "\n",
    "3. **Unstructured vs. Structured:** *Are the time series variables unstructured or structured?*\n",
    "\n",
    "```\n",
    "- The variables are structured; numeric and tabular\n",
    "```\n",
    "\n",
    "4. **Regression vs. Classification:** *Are you working on a regression or classification predictive modeling* *problem? What are some alternate ways to frame your time series forecasting problem?*\n",
    "\n",
    "```\n",
    "- This is a regression modeling problem, predicting numerical values(no of ICU beds in next week)\n",
    "```\n",
    "\n",
    "5. **Univariate vs. Multivariate:**  *Are you working on a univariate or multivariate time series problem?*\n",
    "\n",
    "```\n",
    "- It is a univariate problem; forecasting only on occupied beds.\n",
    "```\n",
    "\n",
    "6. **Single-step vs. Multi-step:** *Do you require a single-step or a multi-step forecast?*\n",
    "\n",
    "```\n",
    "- This is a single-step forecast, predicting only the next 1 week of ICU occupancy.\n",
    "```\n",
    "\n",
    "7. **Static vs. Dynamic:** *Do you require a static or a dynamically updated model?*\n",
    "\n",
    "```\n",
    "- Curently our model is Static but future improvements involve dynamic so it can retrain using new admissions/discharges.\n",
    "```\n",
    "\n",
    "8. **Contiguous vs. Discontiguous:** *Are your observations contiguous or discontiguous?*\n",
    "\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_excel('/content/drive/MyDrive/Capstone/Dataset1.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in our dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset Contains 12 columns.\n",
    "\n",
    "- The date column is in daily format, we will need to aggregrate it to weekly for our forecasting problem\n",
    "\n",
    "- The columns relevant to the problem are: Date, available_ped_icu_beds, total_ped_icu_patients which is also the number_of_occupied beds, and the total_ped_icu_beds.\n",
    "\n",
    "- The reason for using pediatrics data is cause the count is low and can be related to the African context although still too large.\n",
    "\n",
    "- We can drop the rest of our columns, but first let's understand our dataset but getting the info and general statics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Our Dataset(descriptive statistics and info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns we won't need\n",
    "df.drop(['_id', 'adult_icu_crci_patients', 'adult_icu_non_crci_patients',\n",
    "                 'available_adult_icu_beds', 'total_adult_icu_patients', 'total_adult_icu_beds',\n",
    "                 'ped_icu_crci_patients', 'ped_icu_non_crci_patients'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information of our data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have no null values in all our columns and only the date is an object, the rest are integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive analysis\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1651 rowss,\n",
    "\n",
    "- The mean daily occupancy is approximately **73** patients\n",
    "\n",
    "- The range of icu bed occupancy is from **29 to 186**\n",
    "\n",
    "- The ICU bed capacity is from 89- 154 beds.\n",
    "\n",
    "- Let's rename *total_ped_icu_patients* to *occupied_ped_beds*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column total_ped_icu_patients\n",
    "df.rename(columns={'total_ped_icu_patients': 'occupied_ped_icu_beds'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "- Let's visualise our data to understand the trends, and patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set date index\n",
    "df = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort date values\n",
    "df = df.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily occupancy series\n",
    "occupied_beds_daily_series = df['occupied_ped_icu_beds'].asfreq('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily data - ONLY occupied beds\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(df['occupied_ped_icu_beds'], label='Daily Pediatric ICU Patients') # Plotting only the occupied beds column\n",
    "plt.title(\"Daily Pediatric ICU Occupancy\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Occupied Beds\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a noticeable increase in bed occupancy from 2020 through early 2023 then followed by a stable decline period.\n",
    "\n",
    "There are also spikes and dips seen but it is too noisy to detect regular patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the daily time series to specifically look for a weekly pattern(7 day period)\n",
    "decomposition_daily_weekly_seasonality = seasonal_decompose(occupied_beds_daily_series, model='additive', period=7)\n",
    "\n",
    "# Plot the results of the daily decomposition with weekly seasonality\n",
    "fig_daily_weekly = decomposition_daily_weekly_seasonality.plot()\n",
    "fig_daily_weekly.set_size_inches(12, 8)\n",
    "plt.suptitle('Seasonal Decomposition of Daily ICU Occupancy (Weekly Seasonality)', y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the trend, seasonality of ICU bed occupancy, It is however too noisy to intergrate, we will aggregate to monthly and weekly and gain insights from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pediatric ICU Capacity vs. Occupancy Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Capacity vs Occupancy\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot total occupied beds\n",
    "plt.plot(df.index, df['occupied_ped_icu_beds'], label='Occupied Beds', color='blue')\n",
    "\n",
    "# Plot available beds\n",
    "plt.plot(df.index, df['available_ped_icu_beds'], label='Available Beds', color='green', linestyle='--')\n",
    "\n",
    "# Plot total beds\n",
    "plt.plot(df.index, df['total_ped_icu_beds'], label='Total Capacity', color='red', linestyle=':')\n",
    "\n",
    "plt.title(\"Pediatric ICU Capacity vs. Occupancy Over Time (Daily Data)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Beds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This plot visualizes the daily trends in pediatric ICU bed occupancy relative to available beds and total capacity.\n",
    "\n",
    "- The **blue line** represents the **Occupied Beds**, showing the daily number of pediatric ICU patients.\n",
    "- The **green dashed line** represents the **Available Beds**, indicating the number of empty beds.\n",
    "- The **red dotted line** represents the **Total Capacity**, showing the maximum number of pediatric ICU beds.\n",
    "\n",
    "From this plot, we can observe:\n",
    "\n",
    "- A data anomaly was identified where **occupied beds temporarily exceeded total capacity.** We will correct this by capping occupied beds at the total capacity for the affected days.\n",
    "\n",
    "- Assuming total capacity remains constant, there is an **inverse relationship** between occupied and available beds. As the number of Occupied Beds increases, the number of Available Beds generally decreases. Conversely, as Occupied Beds decrease, Available Beds tend to increase.\n",
    "\n",
    "- The plot also shows that the Total Capacity has changed over the observed period, which directly impacts the number of available beds (Available Beds = Total Capacity - Occupied Beds).\n",
    "\n",
    "- Periods where occupied beds are close to or equal to total capacity (and available beds are close to zero), highlighting periods of high utilization or potential strain on resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Anomalies( Where occupied beds exceeds the total beds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check anomaly where occupied beds exceed total capacity\n",
    "anomalies = df[df['occupied_ped_icu_beds'] > df['total_ped_icu_beds']]\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will cap those anomalies at total_ped_icu_beds\n",
    "df.loc[anomalies.index, 'occupied_ped_icu_beds'] = df.loc[anomalies.index, 'total_ped_icu_beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupied_beds_daily_series = df['occupied_ped_icu_beds'].asfreq('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Occupancy Vs Capacity after capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the relevant daily data to weekly frequency (mean)\n",
    "weekly_capacity_occupancy = df[['occupied_ped_icu_beds', 'available_ped_icu_beds', 'total_ped_icu_beds']].resample('W').mean()\n",
    "\n",
    "# Plot Weekly Capacity vs Occupancy\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot weekly occupied beds\n",
    "plt.plot(weekly_capacity_occupancy.index, weekly_capacity_occupancy['occupied_ped_icu_beds'], label='Weekly Occupied Beds (Mean)', color='blue')\n",
    "\n",
    "# Plot weekly available beds\n",
    "plt.plot(weekly_capacity_occupancy.index, weekly_capacity_occupancy['available_ped_icu_beds'], label='Weekly Available Beds (Mean)', color='green', linestyle='--')\n",
    "\n",
    "# Plot weekly total beds\n",
    "plt.plot(weekly_capacity_occupancy.index, weekly_capacity_occupancy['total_ped_icu_beds'], label='Weekly Total Capacity (Mean)', color='red', linestyle=':')\n",
    "\n",
    "plt.title(\"Weekly Pediatric ICU Capacity vs. Occupancy Over Time (Mean)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Number of Beds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot visualizes the **weekly average** trends in pediatric ICU bed occupancy relative to available beds and total capacity.\n",
    "\n",
    "By **aggregating the daily data to a weekly frequency**, some of the day-to-day noise is smoothed out, making it easier to observe longer-term patterns.\n",
    "\n",
    "- The **blue line** represents the **Weekly Average Occupied Beds**, showing the average number of pediatric ICU patients per week.\n",
    "- The **green dashed line** represents the **Weekly Average Available Beds**, indicating the average number of empty beds per week.\n",
    "- The **red dotted line** represents the **Weekly Average Total Capacity**, showing the average maximum number of pediatric ICU beds per week.\n",
    "\n",
    "From this weekly plot, we can observe:\n",
    "\n",
    "- As the Weekly Average Occupied Beds increases, the Weekly Average Available Beds generally decreases, and vice versa, reflecting the inverse relationship (Available Beds = Total Capacity - Occupied Beds).\n",
    "\n",
    "\n",
    "- Changes in the weekly average total capacity of pediatric ICU beds over the observed period( early 2023).\n",
    "\n",
    "- Periods where the average occupied beds are close to the average total capacity (and average available beds are close to zero), highlighting weeks of consistently high utilization or potential strain on resources. This indicates times when the ICU was operating near its capacity. Seen in early 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample weekly and monthly frequency\n",
    "weekly_series = occupied_beds_daily_series.resample('W').mean()\n",
    "monthly_series = occupied_beds_daily_series.resample('ME').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition on the weekly time series using an additive model\n",
    "decomposition_weekly = seasonal_decompose(weekly_series, model='additive')\n",
    "\n",
    "# Perform seasonal decomposition on the monthly time series using an additive model\n",
    "decomposition_monthly = seasonal_decompose(monthly_series, model='additive')\n",
    "\n",
    "# Plot the results of the weekly seasonal decomposition\n",
    "fig_weekly = decomposition_weekly.plot()\n",
    "fig_weekly.set_size_inches(12, 8)\n",
    "plt.suptitle('Seasonal Decomposition of Weekly Pediatric ICU Occupancy', y=1.02) # Add a main title to the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some seasonal trends in weekly ICU occupancy.\n",
    "\n",
    "There are short cycles and flactuactions but patterns are less regular.\n",
    "\n",
    "this likely appears due to local outbreaks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Seasonal Decomposition for Monthly Series ---\n",
    "# Decompose the monthly series (looking for yearly seasonality within monthly data)\n",
    "decomposition_monthly = seasonal_decompose(monthly_series, model='additive') # period defaults to 12 for monthly data\n",
    "\n",
    "fig_monthly = decomposition_monthly.plot()\n",
    "fig_monthly.set_size_inches(12, 8)\n",
    "plt.suptitle('Seasonal Decomposition of Monthly Pediatric ICU Occupancy', y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong seasonal pattern in ICU occupancy monthly. ICU occupancy rises and falls at similar times each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF for the weekly series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot Autocorrelation Function (ACF) for the weekly time series\n",
    "plot_acf(weekly_series, ax=plt.gca(), lags=40)\n",
    "plt.title(\"ACF (Autocorrelation) - Weekly\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot Partial Autocorrelation Function (PACF) for the weekly time series\n",
    "plot_pacf(weekly_series, ax=plt.gca(), lags=40, method='ywm')\n",
    "plt.title(\"PACF (Partial Autocorrelation) - Weekly\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Autocorrelation function shows how the current value in the time series is correlated with the past values(lags)\n",
    "\n",
    "All the lags have a high positive correlation. This suggests that values this week are highly dependent on previus weeks.\n",
    "\n",
    "The values decay gradually indicating a trend with no clear spikes.\n",
    "\n",
    "The ACF does not cut off sharply — instead, it tails off gradually. This is typical of **non-stationary** time series (i.e., the mean and variance change over time) and make require differencing to make it stationary for modeling.\n",
    "\n",
    "PACF isolates the direct effect of a past value on the current value (i.e., it removes indirect influences from intermediate lags).\n",
    "\n",
    "Big spike at lag 1, then all other lags are small and mostly within the confidence interval (the blue shaded region).This suggests that the direct relationship is mostly with the previous week, and other weeks do not directly influence the current week as much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform the Augmented Dickey-Fuller test\n",
    "result = adfuller(weekly_series)\n",
    "\n",
    "# Print the results\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# Interpret the result\n",
    "if result[1] <= 0.05:\n",
    "    print(\"\\nReject the null hypothesis (H0), the time series is stationary.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject the null hypothesis (H0), the time series is non-stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have confirmed that our time series is non-stationary, before we build our first ARIMA model, we will need to perform differencing as most time series models including ARIMA require stationarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differencing\n",
    "\n",
    "Transformation applied to make time series stationary.\n",
    "\n",
    "We will perform first order differencing and rerun the ADF to see if it is stationary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform first-order differencing\n",
    "weekly_series_diff = weekly_series.diff().dropna()\n",
    "\n",
    "# Plot the differenced series\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(weekly_series_diff)\n",
    "plt.title(\"Differenced Weekly Pediatric ICU Occupancy\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Differenced Occupancy\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perform ADF test on the differenced series to confirm stationarity\n",
    "result_diff = adfuller(weekly_series_diff)\n",
    "\n",
    "print('\\nADF Statistic (Differenced Series): %f' % result_diff[0])\n",
    "print('p-value (Differenced Series): %f' % result_diff[1])\n",
    "print('Critical Values (Differenced Series):')\n",
    "for key, value in result_diff[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "if result_diff[1] <= 0.05:\n",
    "    print(\"\\nReject the null hypothesis (H0), the differenced time series is stationary.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject the null hypothesis (H0), the differenced time series is non-stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF for the differenced weekly series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_acf(weekly_series_diff, ax=plt.gca(), lags=40)\n",
    "plt.title(\"ACF (Autocorrelation) - Differenced Weekly\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_pacf(weekly_series_diff, ax=plt.gca(), lags=40, method='ywm')\n",
    "plt.title(\"PACF (Partial Autocorrelation) - Differenced Weekly\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual dofferencing is done to understand what values are fit pdq for our ARIMA model.\n",
    "\n",
    "We have alreday determined that d=1 - Because we applied 1st order differencing.\n",
    "\n",
    "ACF plot shows a significant spike at Lag 1, and then the values quickly drop and mostly fall within the blue shaded confidence interval, suggesting possible Moving Average(q) as 1\n",
    "\n",
    "PACF also cuts off at 1, so a possible starting point is aso 1.\n",
    "\n",
    "That's where we will start from in our ARIMA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix of Weekly Pediatric ICU Occupancy and Lagged Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the weekly series and its lags\n",
    "weekly_series_corr_df = pd.DataFrame({'weekly_series': weekly_series})\n",
    "lags_for_corr = [1, 2, 3, 4] # You can adjust the number of lags\n",
    "\n",
    "for lag in lags_for_corr:\n",
    "    weekly_series_corr_df[f'lag_{lag}'] = weekly_series_corr_df['weekly_series'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values introduced by lagging\n",
    "weekly_series_corr_df = weekly_series_corr_df.dropna()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = weekly_series_corr_df.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Correlation Matrix of Weekly Pediatric ICU Occupancy and Lagged Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This heatmap illustrates the linear correlation between the weekly average pediatric ICU occupancy and its values from previous weeks (lags).\n",
    "\n",
    "From this heatmap, we can see:\n",
    "\n",
    "- Strong positive correlations for the immediate past lags (e.g., `lag_1`, `lag_2`), confirming that recent weekly occupancy is highly predictive of the current week's occupancy.\n",
    "\n",
    "- The correlation generally decreases as the lag increases, indicating that the relationship weakens with time.\n",
    "\n",
    "- The diagonal shows perfect correlation (1.00) as it's the correlation of a variable with itself.\n",
    "\n",
    "This visualization reinforces the time-dependent nature of the data and supports the use of models that leverage past values for forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of weeks in the weekly series to how to split\n",
    "print(f\"Total number of weeks in the dataset: {len(weekly_series)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the full dataset then use ARIMA to forecasr 1 week\n",
    "model = ARIMA(weekly_series, order=(1, 1, 1)) # Baseline model\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print AIC and BIC from the fitted model\n",
    "print(f\"AIC: {model_fit.aic}\")\n",
    "print(f\"BIC: {model_fit.bic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "AIC(Akaike Information Criterion) and BIC(Bayesian Information Criterion.)- These are measures to compare different models and help select best model among a set.\n",
    "\n",
    "We will use these to evaluate the best model of ARIMA's models that we will parameter tune in order to find which model can accurately predict one single datepoint( 1 week forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast and evaluate\n",
    "next_week_forecast = model_fit.forecast(steps=1)\n",
    "\n",
    "print(\"Forecast for the next week:\")\n",
    "print(next_week_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling Forecast evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPlit into train and test(test with prev 20 weeks)\n",
    "# For arima we don't manual lag features\n",
    "train = weekly_series[:-20]\n",
    "test = weekly_series[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Define the ARIMA order\n",
    "arima_order = (1, 1, 1)\n",
    "\n",
    "# List to store forecasts and actual values\n",
    "history = [x for x in train] # Start history with the training data\n",
    "predictions = list()\n",
    "actuals = list()\n",
    "\n",
    "# Walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    # Fit the model on the historical data (train + previous test data points)\n",
    "    model = ARIMA(history, order=arima_order)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Forecast the next step (1-week ahead)\n",
    "    yhat = model_fit.forecast(steps=1)[0]\n",
    "\n",
    "    # Store the forecast and the actual value\n",
    "    predictions.append(yhat)\n",
    "    actual = test[t]\n",
    "    actuals.append(actual)\n",
    "\n",
    "    # Add the actual observation to the history for the next iteration\n",
    "    history.append(actual)\n",
    "\n",
    "    print(f'Week {t+1}/{len(test)}: Predicted={yhat:.3f}, Expected={actual:.3f}')\n",
    "\n",
    "# Evaluate the performance on the rolling forecasts\n",
    "mse_weekly = mean_squared_error(actuals, predictions)\n",
    "rmse_weekly = sqrt(mse_weekly)\n",
    "mae_weekly = mean_absolute_error(actuals, predictions)\n",
    "r2_weekly = r2_score(actuals, predictions)\n",
    "\n",
    "\n",
    "print('\\n--- Rolling Forecast Evaluation Metrics ---')\n",
    "print(f'MSE: {mse_weekly:.3f}')\n",
    "print(f'RMSE: {rmse_weekly:.3f}')\n",
    "print(f'MAE: {mae_weekly:.3f}')\n",
    "print(f\"R-squared (R2) Score: {r2_weekly}\")\n",
    "\n",
    "\n",
    "# plot the actual vs predicted for the test set\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(test.index, actuals, label='Actual')\n",
    "plt.plot(test.index, predictions, label='Rolling Forecast', linestyle='--')\n",
    "plt.title('Rolling 1-Week Ahead Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Pediatric ICU Occupancy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rolling forecast evaluation demonstrates the model's ability to predict weekly pediatric ICU occupancy one week ahead.\n",
    "\n",
    "\n",
    "The lines showing \"Predicted\" vs. \"Expected\" for each week demonstrate how the model's 1-week ahead forecasts compare to the actual occupancy over a 20-week test period.\n",
    "\n",
    "```\n",
    "The Rolling Forecast Evaluation Metrics (MSE, RMSE, MAE, and R-squared) quantify the accuracy of these predictions.\n",
    "RMSE (5.106) and MAE (3.811) indicate that, on average, the model's weekly predictions were off by about 3.8 to 5.1 beds.\n",
    "```\n",
    "\n",
    "The R-squared score (**0.506**) suggests that the model explains roughly 50.6% of the variation in weekly occupancy during the test period.\n",
    "\n",
    "\n",
    "The model shows a moderate ability to forecast weekly ICU occupancy.\n",
    "\n",
    " The visualization confirms that the forecasts are reasonably close to the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(train.index, train, label='Training')\n",
    "plt.plot(test.index, test, label='Actual')\n",
    "plt.plot(test.index, predictions, label='Forecast', linestyle='--')\n",
    "plt.title(f\"ARIMA Forecast vs Actual (RMSE = {rmse_weekly:.2f})\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Weekly Pediatric ICU Occupancy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the first baseline model\n",
    "model_fit.save('final_arima_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from statsmodels.tsa.arima.model import ARIMA # Import the ARIMA class\n",
    "\n",
    "# Temporarily enable warnings to see potential issues\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define parameter ranges for p, d, q\n",
    "p = d = q = range(0, 3)\n",
    "\n",
    "# We determined d=1 is likely necessary, but let's include 0 and 2 in the search for robustness\n",
    "d = range(0, 3)\n",
    "\n",
    "pdq_combinations = list(itertools.product(p, d, q))\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "best_aic = float('inf')\n",
    "best_order = None\n",
    "best_model = None\n",
    "\n",
    "# Grid search over ARIMA hyperparameters\n",
    "# Fit on the training data for tuning\n",
    "for order in tqdm(pdq_combinations, desc=\"Fitting ARIMA models\"):\n",
    "    try:\n",
    "\n",
    "        model = ARIMA(train, order=order)\n",
    "        model_fit = model.fit()\n",
    "        aic = model_fit.aic\n",
    "        results.append((order, aic))\n",
    "\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_order = order\n",
    "\n",
    "            # store just the best order and AIC\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch errors and print them to understand why fitting failed\n",
    "        print(f\"Error fitting ARIMA model for order {order}: {e}\")\n",
    "        continue # Continue to the next combination even if an error occurs\n",
    "\n",
    "# Display best order\n",
    "print(f\"\\n✅ Best ARIMA order: {best_order} with AIC = {best_aic:.2f}\")\n",
    "\n",
    "# Sort results to see other good models\n",
    "results_df = pd.DataFrame(results, columns=['Order', 'AIC']).sort_values(by='AIC')\n",
    "print(\"\\nTop 5 ARIMA Orders by AIC:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find optimal parameters(p, d, q) for the ARIMA model, a grid search was performed.\n",
    "Various combinations with defined range were tested(0-3) for each.\n",
    "\n",
    "\n",
    "Based on the findings, the **best** performing model according to AIC criterion was order (1, 1, 1), with 1316 AIC score.\n",
    "\n",
    "This indicates that the ARIMA(1, 1, 1) model provides the **best balance** between fitting the training data well and keeping the model relatively simple, compared to the other orders we tested.\n",
    "\n",
    "**NOTE:** The differencing order (d=1) is **consistent** with our earlier finding that the time series required **first-order differencing** to achieve stationarity.\n",
    "\n",
    "While (1, 1, 1) was the top choice by AIC, other orders like (0, 1, 2) and (0, 1, 1) also performed well with similarly low AIC scores.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**NEXT STEPS**\n",
    "\n",
    "Our train dataset appears to be too small, we will try to explore data augmentation techniques in time series and see how that affects our model's performance.\n",
    "\n",
    "- Use the previous 7 days to predict all 7 days in one week.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Step Forecasting\n",
    "\n",
    "We will explore multi-step forecasting on daily occupancy time series to predict the next 7 days, evaluate performance and analyze results.\n",
    "\n",
    "## There are different methods of multi-step forecasting:\n",
    "\n",
    "- Direct method: Developing a model with multiple outputs/ multiple models for each time step\n",
    "\n",
    "\n",
    "- Recursive method: Using one step model multiple times where the prediction for the prior time step is used as input for making a prediction of next time step.\n",
    "\n",
    "\n",
    "We will explore both the direct method and recursive methods to know which one works best.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the daily occupancy series\n",
    "# Check for missing values in daily occupancy\n",
    "missing_values_daily = occupied_beds_daily_series.isnull().sum()\n",
    "print(f\"Missing values in daily occupancy series: \\n{missing_values_daily}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA for multi-step forecasting\n",
    "\n",
    "## Direct Multi-Step Forecasting\n",
    "\n",
    "Train the ARIMA model once on the training data(all data before the last 140 days) then make a single forecast for the next 7 days.\n",
    "\n",
    "Evaluation metrics are calculated by comparing the one 7-day forecast to actual values of those specific 7 days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test for stationarity\n",
    "result_daily_adf = adfuller(occupied_beds_daily_series)\n",
    "print('ADF Statistic: %f' % result_daily_adf[0])\n",
    "print('p-value: %f' % result_daily_adf[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result_daily_adf[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# Interpret the result\n",
    "if result_daily_adf[1] <= 0.05:\n",
    "    print(\"\\nResult: Reject the null hypothesis (H0), the daily time series is stationary.\")\n",
    "else:\n",
    "    print(\"\\nResult: Fail to reject the null hypothesis (H0), the daily time series is non-stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the time serries stationary\n",
    "occupied_beds_daily_series_diff = occupied_beds_daily_series.diff().dropna()\n",
    "\n",
    "# Plot the differenced series\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(occupied_beds_daily_series_diff)\n",
    "plt.title(\"Differenced Daily Pediatric ICU Occupancy\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Differenced Occupancy\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For also the daily occupancy, we find out that it is non stationarity and differencing is needed.\n",
    "\n",
    "The d value will therefore be 1 for the ARIMA order since 1st differencing step was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ADF test on the differenced daily series to confirm stationarity\n",
    "print(\"--- Augmented Dickey-Fuller Test (Differenced Daily Series) ---\")\n",
    "result_daily_diff_adf = adfuller(occupied_beds_daily_series_diff)\n",
    "\n",
    "print('ADF Statistic: %f' % result_daily_diff_adf[0])\n",
    "print('p-value: %f' % result_daily_diff_adf[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result_daily_diff_adf[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# Interpret the result\n",
    "if result_daily_diff_adf[1] <= 0.05:\n",
    "    print(\"\\nResult: Reject the null hypothesis (H0), the differenced daily time series is stationary.\")\n",
    "else:\n",
    "    print(\"\\nResult: Fail to reject the null hypothesis (H0), the differenced daily time series is non-stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF Plots for differenced daily series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_acf(occupied_beds_daily_series_diff, ax=plt.gca(), lags=50)\n",
    "plt.title(\"ACF for Differenced Daily Pediatric ICU Occupancy\")\n",
    "\n",
    "# Plot Partial Autocorrelation Function (PACF) for the DIFFERENCED daily time series\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_pacf(occupied_beds_daily_series_diff.dropna(), ax=plt.gca(), lags=50, method='ywm')\n",
    "plt.title(\"PACF (Partial Autocorrelation) - Differenced Daily\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a strong spike in PACF that is greater than 0.5, suggesting string autoregressive effect from previous day.\n",
    "\n",
    "PACF(direct correlation between lags) then drops after lag 2 to lag 6 then the rest hover around zeros, so the reasonable range of p should be between 1-6.\n",
    "\n",
    "\n",
    "For the ACF, there is also a significant spike at lag 1 and then drop in lags 2-6 where the other lags are now within the blue line.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA for multi-step forecasting\n",
    "# Begin with order (1,1,1)\n",
    "arima_order_daily = (1, 1, 1)\n",
    "\n",
    "# using last 140 days for testing\n",
    "test_set_size_daily_arima = 140\n",
    "train_daily = occupied_beds_daily_series[:-test_set_size_daily_arima]\n",
    "test_daily = occupied_beds_daily_series[-test_set_size_daily_arima:]\n",
    "\n",
    "# Fit the ARIMA model on the daily training data\n",
    "model_daily_arima = ARIMA(train_daily, order=arima_order_daily)\n",
    "model_daily_arima_fit = model_daily_arima.fit()\n",
    "\n",
    "# Generate a multi-step forecast for the next 7 days\n",
    "forecast_steps_daily = 7\n",
    "multi_step_forecast_daily = model_daily_arima_fit.forecast(steps=forecast_steps_daily)\n",
    "\n",
    "print(f\"Multi-step forecast for the next {forecast_steps_daily} days:\")\n",
    "print(multi_step_forecast_daily)\n",
    "\n",
    "# Optional: Plot the forecast against the actual values for the forecast period\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(train_daily.index, train_daily, label='Training Data')\n",
    "plt.plot(test_daily.index[:forecast_steps_daily], test_daily[:forecast_steps_daily], label='Actual (Test Data)')\n",
    "plt.plot(multi_step_forecast_daily.index, multi_step_forecast_daily, label='ARIMA Multi-Step Forecast', linestyle='--')\n",
    "plt.title(f'ARIMA ({arima_order_daily}) Multi-Step Daily Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Pediatric ICU Occupancy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_forecast_period = test_daily[:len(multi_step_forecast_daily)]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse_arima_daily = mean_squared_error(actual_forecast_period, multi_step_forecast_daily)\n",
    "rmse_arima_daily = sqrt(mse_arima_daily)\n",
    "mae_arima_daily = mean_absolute_error(actual_forecast_period, multi_step_forecast_daily)\n",
    "r2_arima_daily = r2_score(actual_forecast_period, multi_step_forecast_daily)\n",
    "\n",
    "print(\"--- Multi-Step Daily ARIMA Forecast Evaluation Metrics ---\")\n",
    "print(f'MSE: {mse_arima_daily:.3f}')\n",
    "print(f'RMSE: {rmse_arima_daily:.3f}')\n",
    "print(f'MAE: {mae_arima_daily:.3f}')\n",
    "print(f\"R-squared (R2) Score: {r2_arima_daily:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-step forecast is direct where each of the days being forecasted are independent and no relation between them.\n",
    "\n",
    "It performs poorly with a high MSE of **78.071** and an extremely low R2 score of **-4.761**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning for ARIMA model\n",
    "# Define parameter combinations to try\n",
    "p_values = [0, 1, 2, 3]\n",
    "d_values = [1]  # We know d=1 works well from our previous analysis\n",
    "q_values = [0, 1, 2, 3]\n",
    "\n",
    "# Initialize variables to store best model information\n",
    "best_aic = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "best_metrics = None\n",
    "\n",
    "# Grid search for best parameters\n",
    "print(\"Starting parameter tuning...\")\n",
    "for p in p_values:\n",
    "    for d in d_values:\n",
    "        for q in q_values:\n",
    "            try:\n",
    "                # Fit model with current parameters\n",
    "                model = ARIMA(train_daily, order=(p, d, q))\n",
    "                model_fit = model.fit()\n",
    "\n",
    "                # Make forecast\n",
    "                forecast = model_fit.forecast(steps=forecast_steps_daily)\n",
    "\n",
    "                # Calculate metrics\n",
    "                mse = mean_squared_error(actual_forecast_period, forecast)\n",
    "                rmse = sqrt(mse)\n",
    "                mae = mean_absolute_error(actual_forecast_period, forecast)\n",
    "                r2 = r2_score(actual_forecast_period, forecast)\n",
    "\n",
    "                # Store model if it's better than current best\n",
    "                if model_fit.aic < best_aic:\n",
    "                    best_aic = model_fit.aic\n",
    "                    best_params = (p, d, q)\n",
    "                    best_model = model_fit\n",
    "                    best_metrics = {\n",
    "                        'MSE': mse,\n",
    "                        'RMSE': rmse,\n",
    "                        'MAE': mae,\n",
    "                        'R2': r2\n",
    "                    }\n",
    "                print(f\"ARIMA({p},{d},{q}) - AIC: {model_fit.aic:.2f}\")\n",
    "            except:\n",
    "                print(f\"ARIMA({p},{d},{q}) - Failed to fit\")\n",
    "                continue\n",
    "\n",
    "print(\"\\nBest Model Results:\")\n",
    "print(f\"Best ARIMA parameters: {best_params}\")\n",
    "print(f\"Best AIC: {best_aic:.2f}\")\n",
    "print(\"\\nBest Model Metrics:\")\n",
    "print(f\"MSE: {best_metrics['MSE']:.3f}\")\n",
    "print(f\"RMSE: {best_metrics['RMSE']:.3f}\")\n",
    "print(f\"MAE: {best_metrics['MAE']:.3f}\")\n",
    "print(f\"R-squared (R2) Score: {best_metrics['R2']:.3f}\")\n",
    "\n",
    "# Generate forecast with best model\n",
    "best_forecast = best_model.forecast(steps=forecast_steps_daily)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': actual_forecast_period,\n",
    "    'Original Model': multi_step_forecast_daily,\n",
    "    'Best Model': best_forecast\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Print detailed comparison table\n",
    "print(\"\\nDetailed Forecast Comparison:\")\n",
    "comparison_df['Original Model Error'] = comparison_df['Actual'] - comparison_df['Original Model']\n",
    "comparison_df['Best Model Error'] = comparison_df['Actual'] - comparison_df['Best Model']\n",
    "print(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Retrain the model with best parameters\n",
    "final_model = ARIMA(train_daily, order=best_params)  # using the best_params we found\n",
    "final_model_fit = final_model.fit()\n",
    "\n",
    "# 2. Make predictions with the final model\n",
    "final_forecast = final_model_fit.forecast(steps=forecast_steps_daily)\n",
    "\n",
    "# 3. Evaluate the final model\n",
    "final_metrics = {\n",
    "    'MSE': mean_squared_error(actual_forecast_period, final_forecast),\n",
    "    'RMSE': sqrt(mean_squared_error(actual_forecast_period, final_forecast)),\n",
    "    'MAE': mean_absolute_error(actual_forecast_period, final_forecast),\n",
    "    'R2': r2_score(actual_forecast_period, final_forecast)\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Model Evaluation Metrics:\")\n",
    "print(f\"ARIMA{best_params} Metrics:\")\n",
    "print(f\"MSE: {final_metrics['MSE']:.3f}\")\n",
    "print(f\"RMSE: {final_metrics['RMSE']:.3f}\")\n",
    "print(f\"MAE: {final_metrics['MAE']:.3f}\")\n",
    "print(f\"R-squared (R2) Score: {final_metrics['R2']:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 6. Visualize final model performance\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(train_daily.index, train_daily, label='Training Data', color='blue', alpha=0.5)\n",
    "plt.plot(actual_forecast_period.index, actual_forecast_period, label='Actual', color='green')\n",
    "plt.plot(final_forecast.index, final_forecast,\n",
    "         label=f'Final Model ARIMA{best_params}', color='purple', linestyle='--')\n",
    "\n",
    "plt.title('Final ARIMA Model: Training, Actual, and Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Pediatric ICU Occupancy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with parameter tuning, the performance is still very poor. Although the best model with a low AIC score of **9107.71**, it still has a higher MSE of **80.874**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Multi-Step Forecasting Explained\n",
    "\n",
    "This section implemented a **Rolling Multi-Step Forecasting** approach using an ARIMA model on the daily pediatric ICU occupancy data (`occupied_beds_daily_series`).\n",
    "\n",
    "Here's a breakdown of the process:\n",
    "\n",
    "-   **Inputs:** The model uses the historical daily pediatric ICU occupancy data as its input time series. Specifically, for each forecast, it trains on a growing history of *actual* past occupancy values.\n",
    "-   **Outputs:** The model forecasts the **next 7 days** of pediatric ICU occupancy in each step.\n",
    "-   **Test Set:** A test set of the **last 140 days** of the daily occupancy data was used for evaluation. This period consists of 20 distinct 7-day windows (140 days / 7 days/forecast window = 20 windows).\n",
    "-   **Rolling Forecast Mechanism:**\n",
    "    1.  The model is initially trained on all daily data *before* the 140-day test set.\n",
    "    2.  It makes a 7-day forecast for the first 7 days of the test set.\n",
    "    3.  The *actual* values for those first 7 days are then added to the model's training history.\n",
    "    4.  The model is re-trained on this expanded history.\n",
    "    5.  It then makes a 7-day forecast for the *next* 7 days of the test set.\n",
    "    6.  This process of forecasting 7 days, adding the actuals to the history, and re-training is repeated sequentially for each of the 20 distinct 7-day windows within the 140-day test set.\n",
    "-   **Evaluation:** The performance metrics (MSE, RMSE, MAE, R2) are calculated by comparing all the stored 7-day forecasts against their corresponding actual values across the entire 140-day test period. This provides a more robust evaluation of the model's performance over time as it incorporates new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the daily occupied beds series\n",
    "time_series = occupied_beds_daily_series.copy()\n",
    "\n",
    "# Define the forecast horizon (7days)\n",
    "forecast_horizon = 7\n",
    "\n",
    "# Test set\n",
    "test_set_days = 140\n",
    "\n",
    "# Split into train and test sets for rolling forecast\n",
    "train_size = len(time_series) - test_set_days\n",
    "train_data = time_series[:train_size].tolist()\n",
    "test_data = time_series[train_size:]\n",
    "\n",
    "# Define the ARIMA order\n",
    "# We will start with (3,1,3) based on the previous tuning on the direct method forecast period\n",
    "arima_order = (3, 1, 3)\n",
    "\n",
    "# Lists to store forecasts and actual values\n",
    "rolling_predictions = list()\n",
    "rolling_actuals = list()\n",
    "rolling_forecast_dates = list()\n",
    "\n",
    "print(f\"Starting rolling {forecast_horizon}-day ahead forecast...\")\n",
    "\n",
    "# Implement rolling forecast\n",
    "history = train_data.copy()\n",
    "test_index = test_data.index\n",
    "\n",
    "# Loop through the test set in steps of the forecast_horizon\n",
    "for i in range(0, len(test_data), forecast_horizon):\n",
    "    # Define the chunk of actual data for the current forecast window\n",
    "    actual_window = test_data.iloc[i : i + forecast_horizon]\n",
    "\n",
    "    # Check if the window is complete (has forecast_horizon elements)\n",
    "    if len(actual_window) < forecast_horizon:\n",
    "        print(f\"Reached end of test data. Processing last incomplete window of size {len(actual_window)}.\")\n",
    "        # Adjust forecast steps if the last window is incomplete\n",
    "        current_forecast_steps = len(actual_window)\n",
    "    else:\n",
    "        current_forecast_steps = forecast_horizon\n",
    "\n",
    "    if current_forecast_steps == 0:\n",
    "        break # Stop if there's no data left to forecast\n",
    "\n",
    "    try:\n",
    "        # Fit the model on the historical data\n",
    "        # ARIMA requires a pandas Series or numpy array, list needs conversion\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Forecast the next 'current_forecast_steps' steps\n",
    "        forecast = model_fit.forecast(steps=current_forecast_steps)\n",
    "\n",
    "        # Store the forecasts and actual values\n",
    "        rolling_predictions.extend(forecast.tolist()) # Convert forecast Series to list\n",
    "        rolling_actuals.extend(actual_window.tolist())\n",
    "        rolling_forecast_dates.extend(actual_window.index.tolist()) # Store dates\n",
    "\n",
    "        # Add the actual observations from the current window to the history for the next iteration\n",
    "        history.extend(actual_window.tolist())\n",
    "\n",
    "        print(f'Forecasted {current_forecast_steps} steps starting from {actual_window.index[0].date()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during rolling forecast for window starting {actual_window.index[0].date()}: {e}\")\n",
    "        history.extend(actual_window.tolist())\n",
    "        # Append NaNs for predictions if forecasting failed\n",
    "        rolling_predictions.extend([np.nan] * current_forecast_steps)\n",
    "        rolling_actuals.extend(actual_window.tolist())\n",
    "        rolling_forecast_dates.extend(actual_window.index.tolist())\n",
    "        continue # Continue to the next window\n",
    "\n",
    "\n",
    "print(\"\\nRolling forecast complete.\")\n",
    "\n",
    "# Convert results to pandas Series for easier handling and plotting\n",
    "rolling_actuals_series = pd.Series(rolling_actuals, index=rolling_forecast_dates)\n",
    "rolling_predictions_series = pd.Series(rolling_predictions, index=rolling_forecast_dates)\n",
    "\n",
    "# Evaluate the performance of the rolling forecast\n",
    "# Drop NaNs from predictions before calculating metrics if any errors occurred\n",
    "valid_indices = rolling_predictions_series.dropna().index\n",
    "actuals_for_metrics = rolling_actuals_series.loc[valid_indices]\n",
    "predictions_for_metrics = rolling_predictions_series.dropna()\n",
    "\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    mse_rolling = mean_squared_error(actuals_for_metrics, predictions_for_metrics)\n",
    "    rmse_rolling = sqrt(mse_rolling)\n",
    "    mae_rolling = mean_absolute_error(actuals_for_metrics, predictions_for_metrics)\n",
    "    r2_rolling = r2_score(actuals_for_metrics, predictions_for_metrics)\n",
    "\n",
    "    print('\\n--- Rolling Multi-Step Forecast Evaluation Metrics ---')\n",
    "    print(f'MSE: {mse_rolling:.3f}')\n",
    "    print(f'RMSE: {rmse_rolling:.3f}')\n",
    "    print(f'MAE: {mae_rolling:.3f}')\n",
    "    print(f\"R-squared (R2) Score: {r2_rolling:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid predictions were generated for evaluation.\")\n",
    "\n",
    "\n",
    "# Plot the actual vs rolling multi-step forecast\n",
    "plt.figure(figsize=(14, 5))\n",
    "# Plot the training data\n",
    "plt.plot(time_series.index[:train_size], time_series[:train_size], label='Training Data', color='blue', alpha=0.6)\n",
    "# Plot the actual test data\n",
    "plt.plot(test_index, test_data, label='Actual Test Data', color='green')\n",
    "# Plot the rolling forecast predictions (only valid ones)\n",
    "plt.plot(predictions_for_metrics.index, predictions_for_metrics, label=f'Rolling {forecast_horizon}-Day Forecast (ARIMA{arima_order})', color='red', linestyle='--')\n",
    "\n",
    "plt.title(f'Rolling {forecast_horizon}-Day Ahead Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Pediatric ICU Occupancy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print the detailed comparison table for the rolling forecast period\n",
    "print(\"\\nDetailed Rolling Forecast Results:\")\n",
    "rolling_comparison_df = pd.DataFrame({\n",
    "    'Actual': rolling_actuals_series,\n",
    "    'Predicted': rolling_predictions_series\n",
    "}).dropna() # Only show rows where a prediction was made\n",
    "\n",
    "print(rolling_comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Rolling forecast, the baseline model performs better than the direct one.\n",
    "\n",
    "With:\n",
    "\n",
    "- MSE: **36.112**,\n",
    "- RMSE: **6.009**\n",
    "- MAE: **4.771**\n",
    "- R-squared (R2) Score: **0.493**\n",
    "\n",
    "\n",
    "A strong positive r2 score of 0.493 explains that the model's variance 49.3% of variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools # Import the itertools library\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Use the daily occupied beds series\n",
    "time_series = occupied_beds_daily_series.copy()\n",
    "\n",
    "# Define the forecast horizon (number of steps to forecast at each iteration)\n",
    "forecast_horizon = 7\n",
    "\n",
    "# Define the size of the test set (last 20 weeks, which is 20 * 7 days)\n",
    "test_set_days = 20 * 7 # 140 days\n",
    "\n",
    "# Split into train and test sets for rolling forecast (initial split)\n",
    "train_size = len(time_series) - test_set_days\n",
    "initial_train_data = time_series[:train_size]\n",
    "test_data = time_series[train_size:]\n",
    "\n",
    "# Define parameter ranges for p, d, q\n",
    "p_values = range(0, 4)\n",
    "d_values = [1]        # Based on our stationarity test result\n",
    "q_values = range(0, 4) # Example: 0, 1, 2, 3\n",
    "\n",
    "pdq_combinations = list(itertools.product(p_values, d_values, q_values))\n",
    "\n",
    "# Store results\n",
    "results_rolling_tuning = []\n",
    "# We will optimize based on RMSE, aiming for the lowest value\n",
    "best_rmse_rolling = float('inf')\n",
    "best_order_rolling = None\n",
    "best_metrics_rolling = None\n",
    "\n",
    "print(f\"Starting parameter tuning for Rolling Multi-Step ARIMA ({forecast_horizon}-day forecast, {test_set_days}-day test set)...\")\n",
    "\n",
    "# Grid search over ARIMA hyperparameters\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for order in tqdm(pdq_combinations, desc=\"Fitting Rolling ARIMA models\"):\n",
    "    try:\n",
    "        # Lists to store forecasts and actual values for the current order\n",
    "        rolling_predictions_current = list()\n",
    "        rolling_actuals_current = list()\n",
    "        rolling_forecast_dates_current = list()\n",
    "\n",
    "        # Implement rolling forecast for the current order\n",
    "        # Start history with the initial training data (converted to list for easy appending)\n",
    "        history = initial_train_data.tolist()\n",
    "        test_index = test_data.index\n",
    "\n",
    "        # Loop through the test set in steps of the forecast_horizon\n",
    "        for i in range(0, len(test_data), forecast_horizon):\n",
    "            actual_window = test_data.iloc[i : i + forecast_horizon]\n",
    "\n",
    "            if len(actual_window) < forecast_horizon:\n",
    "                current_forecast_steps = len(actual_window)\n",
    "            else:\n",
    "                current_forecast_steps = forecast_horizon\n",
    "\n",
    "            if current_forecast_steps == 0:\n",
    "                break\n",
    "\n",
    "            # Fit the model on the historical data\n",
    "            model = ARIMA(history, order=order)\n",
    "            model_fit = model.fit()\n",
    "\n",
    "            # Forecast the next 'current_forecast_steps' steps\n",
    "            forecast = model_fit.forecast(steps=current_forecast_steps)\n",
    "\n",
    "            # Store results for this window\n",
    "            rolling_predictions_current.extend(forecast.tolist())\n",
    "            rolling_actuals_current.extend(actual_window.tolist())\n",
    "            rolling_forecast_dates_current.extend(actual_window.index.tolist())\n",
    "\n",
    "            # Add the actual observations from the current window to the history\n",
    "            history.extend(actual_window.tolist())\n",
    "\n",
    "        # Convert results to pandas Series for easier handling\n",
    "        rolling_actuals_series_current = pd.Series(rolling_actuals_current, index=rolling_forecast_dates_current)\n",
    "        rolling_predictions_series_current = pd.Series(rolling_predictions_current, index=rolling_forecast_dates_current)\n",
    "\n",
    "        # Evaluate the performance on the rolling forecasts for the current order\n",
    "        valid_indices_current = rolling_predictions_series_current.dropna().index\n",
    "        actuals_for_metrics_current = rolling_actuals_series_current.loc[valid_indices_current]\n",
    "        predictions_for_metrics_current = rolling_predictions_series_current.dropna()\n",
    "\n",
    "        # Only calculate metrics if there are valid predictions\n",
    "        if len(valid_indices_current) > 0:\n",
    "            mse = mean_squared_error(actuals_for_metrics_current, predictions_for_metrics_current)\n",
    "            rmse = sqrt(mse)\n",
    "            mae = mean_absolute_error(actuals_for_metrics_current, predictions_for_metrics_current)\n",
    "            r2 = r2_score(actuals_for_metrics_current, predictions_for_metrics_current)\n",
    "\n",
    "            # Store metrics for this order\n",
    "            metrics_current = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "            results_rolling_tuning.append((order, metrics_current))\n",
    "\n",
    "            # Check if this is the best model based on RMSE\n",
    "            if rmse < best_rmse_rolling:\n",
    "                best_rmse_rolling = rmse\n",
    "                best_order_rolling = order\n",
    "                best_metrics_rolling = metrics_current\n",
    "\n",
    "            print(f\"ARIMA{order} - RMSE: {rmse:.3f}, MAE: {mae:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "        else:\n",
    "             print(f\"ARIMA{order} - No valid predictions generated.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch errors and print them\n",
    "        print(f\"Error fitting or evaluating ARIMA model for order {order}: {e}\")\n",
    "        # Append None or indicate failure in results if desired\n",
    "        results_rolling_tuning.append((order, None))\n",
    "        continue # Continue to the next combination\n",
    "\n",
    "warnings.filterwarnings(\"default\") # Re-enable warnings\n",
    "\n",
    "print(\"\\n--- Parameter Tuning Complete ---\")\n",
    "print(f\"✅ Best ARIMA order for Rolling Multi-Step Forecast: {best_order_rolling}\")\n",
    "print(\"\\nBest Model Metrics:\")\n",
    "print(f\"MSE: {best_metrics_rolling['MSE']:.3f}\")\n",
    "print(f\"RMSE: {best_metrics_rolling['RMSE']:.3f}\")\n",
    "print(f\"MAE: {best_metrics_rolling['MAE']:.3f}\")\n",
    "print(f\"R-squared (R2) Score: {best_metrics_rolling['R2']:.3f}\")\n",
    "\n",
    "# Optional: Convert results to DataFrame for easier viewing\n",
    "results_df_rolling_tuning = pd.DataFrame(results_rolling_tuning, columns=['Order', 'Metrics'])\n",
    "if not results_df_rolling_tuning.empty:\n",
    "    # Expand the metrics dictionary into separate columns\n",
    "    metrics_df = results_df_rolling_tuning['Metrics'].apply(pd.Series)\n",
    "    results_df_rolling_tuning = pd.concat([results_df_rolling_tuning['Order'], metrics_df], axis=1)\n",
    "    # Sort by RMSE to see the best models\n",
    "    results_df_rolling_tuning = results_df_rolling_tuning.sort_values(by='RMSE')\n",
    "    print(\"\\nAll Parameter Tuning Results (sorted by RMSE):\")\n",
    "    print(results_df_rolling_tuning)\n",
    "else:\n",
    "    print(\"\\nNo successful model fits during tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the parameter tuning, we see that the best performing order with the lowest mse is (2, 1, 3).\n",
    "\n",
    "We will now use that to retrain our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Use the daily occupied beds series\n",
    "time_series = occupied_beds_daily_series.copy()\n",
    "\n",
    "# Define the forecast horizon (7 days)\n",
    "forecast_horizon = 7\n",
    "\n",
    "# Test set length\n",
    "test_set_days = 140\n",
    "\n",
    "# Split into train and test sets for rolling forecast\n",
    "train_size = len(time_series) - test_set_days\n",
    "train_data = time_series[:train_size].tolist()\n",
    "test_data = time_series[train_size:]\n",
    "\n",
    "# Tuned ARIMA order\n",
    "arima_order = (2, 1, 3)\n",
    "\n",
    "# Lists to store forecasts and actual values\n",
    "rolling_predictions = []\n",
    "rolling_actuals = []\n",
    "rolling_forecast_dates = []\n",
    "\n",
    "print(f\"Starting rolling {forecast_horizon}-day ahead forecast...\")\n",
    "\n",
    "# Start history with training data\n",
    "history = train_data.copy()\n",
    "test_index = test_data.index\n",
    "\n",
    "# Loop through test set in chunks of forecast_horizon\n",
    "for i in range(0, len(test_data), forecast_horizon):\n",
    "    actual_window = test_data.iloc[i: i + forecast_horizon]\n",
    "\n",
    "    if len(actual_window) < forecast_horizon:\n",
    "        print(f\"Reached end of test data. Processing last incomplete window of size {len(actual_window)}.\")\n",
    "        current_forecast_steps = len(actual_window)\n",
    "    else:\n",
    "        current_forecast_steps = forecast_horizon\n",
    "\n",
    "    if current_forecast_steps == 0:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Fit the model\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Forecast the next steps\n",
    "        forecast = model_fit.forecast(steps=current_forecast_steps)\n",
    "\n",
    "        # Store forecasts and actuals\n",
    "        rolling_predictions.extend(forecast.tolist())\n",
    "        rolling_actuals.extend(actual_window.tolist())\n",
    "        rolling_forecast_dates.extend(actual_window.index.tolist())\n",
    "\n",
    "        # Update history\n",
    "        history.extend(actual_window.tolist())\n",
    "\n",
    "        print(f'Forecasted {current_forecast_steps} steps starting from {actual_window.index[0].date()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during rolling forecast for window starting {actual_window.index[0].date()}: {e}\")\n",
    "        history.extend(actual_window.tolist())\n",
    "        rolling_predictions.extend([np.nan] * current_forecast_steps)\n",
    "        rolling_actuals.extend(actual_window.tolist())\n",
    "        rolling_forecast_dates.extend(actual_window.index.tolist())\n",
    "        continue\n",
    "\n",
    "print(\"\\nRolling forecast complete.\")\n",
    "\n",
    "# Save final fitted model on the full history\n",
    "try:\n",
    "    final_model = ARIMA(history, order=arima_order).fit()\n",
    "    joblib.dump(final_model, 'arima_final_model.joblib')\n",
    "    print(\"Final ARIMA model saved as arima_final_model.joblib\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final model: {e}\")\n",
    "\n",
    "# Convert to pandas Series for plotting\n",
    "rolling_actuals_series = pd.Series(rolling_actuals, index=rolling_forecast_dates)\n",
    "rolling_predictions_series = pd.Series(rolling_predictions, index=rolling_forecast_dates)\n",
    "\n",
    "# Evaluate performance\n",
    "valid_indices = rolling_predictions_series.dropna().index\n",
    "actuals_for_metrics = rolling_actuals_series.loc[valid_indices]\n",
    "predictions_for_metrics = rolling_predictions_series.dropna()\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    mse_rolling = mean_squared_error(actuals_for_metrics, predictions_for_metrics)\n",
    "    rmse_rolling = sqrt(mse_rolling)\n",
    "    mae_rolling = mean_absolute_error(actuals_for_metrics, predictions_for_metrics)\n",
    "    r2_rolling = r2_score(actuals_for_metrics, predictions_for_metrics)\n",
    "\n",
    "    print('\\n--- Rolling Multi-Step Forecast Evaluation Metrics ---')\n",
    "    print(f'MSE: {mse_rolling:.3f}')\n",
    "    print(f'RMSE: {rmse_rolling:.3f}')\n",
    "    print(f'MAE: {mae_rolling:.3f}')\n",
    "    print(f\"R-squared (R2) Score: {r2_rolling:.3f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid predictions were generated for evaluation.\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(time_series.index[:train_size], time_series[:train_size], label='Training Data', color='blue', alpha=0.6)\n",
    "plt.plot(test_index, test_data, label='Actual Test Data', color='green')\n",
    "plt.plot(predictions_for_metrics.index, predictions_for_metrics, label=f'Rolling {forecast_horizon}-Day Forecast (ARIMA{arima_order})', color='red', linestyle='--')\n",
    "\n",
    "plt.title(f'Rolling {forecast_horizon}-Day Ahead Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Pediatric ICU Occupancy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: print comparison table\n",
    "print(\"\\nDetailed Rolling Forecast Results:\")\n",
    "rolling_comparison_df = pd.DataFrame({\n",
    "    'Actual': rolling_actuals_series,\n",
    "    'Predicted': rolling_predictions_series\n",
    "}).dropna()\n",
    "\n",
    "print(rolling_comparison_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define your metric dictionaries\n",
    "single_step_metrics = {\n",
    "    'MSE': mse_weekly,\n",
    "    'RMSE': rmse_weekly,\n",
    "    'MAE': mae_weekly,\n",
    "    'R2': r2_weekly\n",
    "}\n",
    "\n",
    "multi_step_metrics = {\n",
    "    'MSE': mse_rolling,\n",
    "    'RMSE': rmse_rolling,\n",
    "    'MAE': mae_rolling,\n",
    "    'R2': r2_rolling\n",
    "}\n",
    "\n",
    "# Combine into a DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Single-Step': single_step_metrics,\n",
    "    'Multi-Step': multi_step_metrics\n",
    "})\n",
    "\n",
    "# Transpose so each row is a model, each column is a metric\n",
    "metrics_df = metrics_df.T\n",
    "\n",
    "# Create bar plot\n",
    "ax = metrics_df.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "plt.title('Evaluation Metrics: Single-Step vs Multi-Step ARIMA Forecasts')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(title='Metric')\n",
    "\n",
    "# Add metric values on top of each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', label_type='edge', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single step ARIMA performs better on MSE, RMSE and MAE. WIth lower values of 26.068, 5.106, 3.811 respectively.\n",
    "\n",
    "Multi step fprecasting performs better on R2 score, there is a bit more variance.\n",
    "\n",
    "The Single-Step ARIMA model performs better overall in terms of lower prediction error, even though the Multi-Step model has a marginally higher R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMAResults\n",
    "\n",
    "# Load the trained ARIMA model\n",
    "model = ARIMAResults.load(\"/content/drive/MyDrive/Capstone/baseline_arima_model.pkl\")\n",
    "\n",
    "# Forecast next week's ICU occupancy\n",
    "forecast = model.forecast(steps=1)[0]\n",
    "\n",
    "# Get latest total ICU beds capacity dynamically from your data\n",
    "#total_ped_icu_beds = df['total_ped_icu_beds'].dropna().iloc[-1]\n",
    "\n",
    "total_ped_icu_beds = 45\n",
    "\n",
    "SURGE_THRESHOLD = 0.8 * total_ped_icu_beds\n",
    "\n",
    "if forecast > SURGE_THRESHOLD:\n",
    "    print(f\"🚨 Surge Alert! Forecast ({forecast:.2f}) exceeds 80% of total ICU capacity ({SURGE_THRESHOLD:.2f})\")\n",
    "else:\n",
    "    print(f\"✅ Forecast is safe. Predicted occupancy: {forecast:.2f}, Threshold: {SURGE_THRESHOLD:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet Model Building\n",
    "\n",
    "Facebook's prophet is a time series forecasting tool to handle time series data that display clear **seasonality**, **trend**, **holiday effects**.\n",
    "\n",
    "Prophet uses direct forecasting, making all future predictions in one step.\n",
    "\n",
    "\n",
    "Decomposes into 3 main components:\n",
    "Trend, seasonality, and holidays/events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare Data for Prophet\n",
    "# Prepare weekly data for Prophet\n",
    "# Prophet requires column names 'ds' (datetime) and 'y' (value)\n",
    "weekly_prophet_df = weekly_series.reset_index()\n",
    "weekly_prophet_df.columns = ['ds', 'y']\n",
    "\n",
    "print(\"Weekly data prepared for Prophet:\")\n",
    "display(weekly_prophet_df.head())\n",
    "print(\"\\nWeekly data info:\")\n",
    "weekly_prophet_df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set\n",
    "train_df = weekly_prophet_df[:-20]\n",
    "test_df = weekly_prophet_df[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Train Prophet on training\n",
    "model = Prophet(\n",
    "    weekly_seasonality=True,\n",
    "    seasonality_mode='additive'\n",
    "\n",
    ")\n",
    "model.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Create future dataframe for 1-week-ahead forecast ---\n",
    "future = model.make_future_dataframe(periods=1, freq='W')\n",
    "forecast = model.predict(future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate forecast against true test value\n",
    "\n",
    "# Extract the single 1-week ahead prediction from the forecast\n",
    "# The forecast DataFrame contains predictions for the entire history + 1 future week.\n",
    "# The 1-week ahead prediction is the last value in the 'yhat' column.\n",
    "y_pred = forecast['yhat'].iloc[-1]\n",
    "\n",
    "# Extract the first actual value from the test set (this is the true value for the week immediately following the training data)\n",
    "y_true = test_df['y'].iloc[0]\n",
    "\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mse_prophet_weekly = mean_squared_error([y_true], [y_pred])\n",
    "rmse_prophet_weekly = np.sqrt(mse_prophet_weekly)\n",
    "mae_prophet_weekly = mean_absolute_error([y_true], [y_pred])\n",
    "\n",
    "# Handle R2 score calculation for a single point comparison\n",
    "try:\n",
    "    # R2 score for a single point is typically NaN or can be problematic\n",
    "    # depending on the implementation and values.\n",
    "    r2 = r2_score([y_true], [y_pred])\n",
    "except ValueError:\n",
    "    r2 = 0 # Set R2 to NaN if calculation fails for a single point\n",
    "\n",
    "# Print metrics\n",
    "print(\"📊 Prophet Evaluation Metrics (1-week forecast):\")\n",
    "print(f\"✅ True value: {y_true:.2f}\")\n",
    "print(f\"✅ Predicted:  {y_pred:.2f}\\n\")\n",
    "print(f\"📈 Mean Squared Error (MSE): {mse_prophet_weekly:.2f}\")\n",
    "print(f\"📈 Root Mean Squared Error (RMSE): {rmse_prophet_weekly:.2f}\")\n",
    "print(f\"📈 Mean Absolute Error (MAE): {mae_prophet_weekly:.2f}\")\n",
    "print(f\"📈 R² Score: {r2:.4f}\")\n",
    "\n",
    "# --- Step 7: Plot Actual vs Forecast ---\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Plot training data\n",
    "plt.plot(train_df['ds'], train_df['y'], label='Train Data')\n",
    "# Plot actual test data (the 20 weeks held out)\n",
    "plt.plot(test_df['ds'], test_df['y'], label='True Future (Test)', color='green', marker='o')\n",
    "# Plot the specific 1-week ahead forecast point\n",
    "plt.plot(forecast['ds'].iloc[-1], y_pred, label='1-Week Ahead Forecast (yhat)', color='red', marker='X', markersize=10)\n",
    "\n",
    "\n",
    "# Add a vertical line to show where the forecast starts\n",
    "plt.axvline(x=test_df['ds'].iloc[0], color='gray', linestyle=':', label='Forecast Start')\n",
    "\n",
    "plt.title('Prophet 1-Week Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('ICU Occupancy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's high MSE of **45.46** indicates poor performance. This is likely because Prophet relies on clear seasonal patterns and holidays to make accurate forecasts. However, our ICU occupancy data lacks explicit markers of known seasonal events (like flu seasons or holidays) that typically drive hospitalizations. Without these recognizable patterns or additional relevant features, Prophet struggles to identify meaningful trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_prophet_df = occupied_beds_daily_series.reset_index()\n",
    "daily_prophet_df.columns = ['ds', 'y']\n",
    "\n",
    "print(\"Daily data prepared for Prophet:\")\n",
    "display(daily_prophet_df.head())\n",
    "print(\"\\nDaily data info:\")\n",
    "daily_prophet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prophet for Multi-Step (Daily) Forecasting ---\n",
    "\n",
    "# Initialize Prophet model for daily data\n",
    "model_daily_prophet = Prophet(daily_seasonality=True,\n",
    "                              weekly_seasonality=True,\n",
    "                              yearly_seasonality=True,\n",
    "                              seasonality_mode='multiplicative'\n",
    "                              )\n",
    "\n",
    "# Fit the model on the daily data\n",
    "model_daily_prophet.fit(daily_prophet_df)\n",
    "\n",
    "print(\"Prophet model fitted for daily data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prophet for Multi-Step (Daily) Forecasting ---\n",
    "\n",
    "# 7 days forecast horizon\n",
    "forecast_horizon_daily = 7\n",
    "\n",
    "# Define the test set size (last 140 days)\n",
    "test_set_days_daily = 140\n",
    "\n",
    "train_daily_prophet = daily_prophet_df[:-test_set_days_daily]\n",
    "test_daily_prophet = daily_prophet_df[-test_set_days_daily:].copy()\n",
    "\n",
    "print(f\"Training data size (days): {len(train_daily_prophet)}\")\n",
    "print(f\"Test data size (days): {len(test_daily_prophet)}\")\n",
    "\n",
    "\n",
    "# Create a future dataframe for the forecast horizon (7 days) after the end of the *entire* dataset for plotting\n",
    "# For evaluation, we will compare the forecast for the test period with the actuals in test_daily_prophet\n",
    "future_daily = model_daily_prophet.make_future_dataframe(periods=forecast_horizon_daily, freq='D')\n",
    "\n",
    "# Generate the multi-step forecast\n",
    "multi_step_forecast_daily_prophet = model_daily_prophet.predict(future_daily)\n",
    "\n",
    "print(f\"\\nMulti-step forecast generated for {forecast_horizon_daily} days:\")\n",
    "# Display the relevant part of the forecast (the future period)\n",
    "display(multi_step_forecast_daily_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(forecast_horizon_daily))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "\n",
    "# We need to align the forecast with the actual test data for evaluation.\n",
    "# The multi_step_forecast_daily_prophet DataFrame contains predictions for the entire history + future.\n",
    "# We are interested in the predictions that correspond to the dates in test_daily_prophet.\n",
    "\n",
    "# Extract the forecast for the test period\n",
    "# Ensure the forecast dates match the test dates\n",
    "forecast_for_test_period = multi_step_forecast_daily_prophet[\n",
    "    multi_step_forecast_daily_prophet['ds'].isin(test_daily_prophet['ds'])\n",
    "].set_index('ds')\n",
    "\n",
    "# Align actual test values with the forecast dates\n",
    "actual_test_values = test_daily_prophet.set_index('ds').loc[forecast_for_test_period.index]['y']\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "# Using'yhat' column from the forecast for predictions\n",
    "predictions_for_metrics = forecast_for_test_period['yhat']\n",
    "\n",
    "mse_prophet_daily = mean_squared_error(actual_test_values, predictions_for_metrics)\n",
    "rmse_prophet_daily = sqrt(mse_prophet_daily)\n",
    "mae_prophet_daily = mean_absolute_error(actual_test_values, predictions_for_metrics)\n",
    "r2_prophet_daily = r2_score(actual_test_values, predictions_for_metrics)\n",
    "\n",
    "print(\"--- Multi-Step Daily Prophet Forecast Evaluation Metrics ---\")\n",
    "print(f'MSE: {mse_prophet_daily:.3f}')\n",
    "print(f'RMSE: {rmse_prophet_daily:.3f}')\n",
    "print(f'MAE: {mae_prophet_daily:.3f}')\n",
    "print(f\"R-squared (R2) Score: {r2_prophet_daily:.3f}\")\n",
    "\n",
    "# Optional: Display a comparison DataFrame for the test period\n",
    "comparison_df_prophet_daily = pd.DataFrame({\n",
    "    'Actual': actual_test_values,\n",
    "    'Prophet Forecast': predictions_for_metrics\n",
    "})\n",
    "print(\"\\nDetailed Daily Forecast Comparison (Test Period):\")\n",
    "display(comparison_df_prophet_daily.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your metric dictionaries\n",
    "single_step_metrics = {\n",
    "    'MSE': mse_prophet_weekly,\n",
    "    'RMSE': rmse_prophet_weekly,\n",
    "    'MAE': mae_prophet_weekly,\n",
    "    'R2': r2\n",
    "}\n",
    "\n",
    "multi_step_metrics = {\n",
    "    'MSE': mse_prophet_daily,\n",
    "    'RMSE': rmse_prophet_daily,\n",
    "    'MAE': mae_prophet_daily,\n",
    "    'R2': r2_prophet_daily\n",
    "}\n",
    "\n",
    "# Combine into a DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Single-Step': single_step_metrics,\n",
    "    'Multi-Step': multi_step_metrics\n",
    "})\n",
    "\n",
    "# Transpose so each row is a model, each column is a metric\n",
    "metrics_df = metrics_df.T\n",
    "\n",
    "# Create bar plot\n",
    "ax = metrics_df.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "plt.title('Evaluation Metrics: Single-Step vs Multi-Step PROPHET Forecasts')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(title='Metric')\n",
    "\n",
    "# Add metric values on top of each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', label_type='edge', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prophet, multi step forecasting outperforms single step forecasting in all the metrics.\n",
    "\n",
    "MSE - **31.810**, rmse **5.640**, mae **4.578**, r2 score-**0.554**\n",
    "as compared to single step with **45.461**, **6.742**, **6.742** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM MODEL BUILDING\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lagged features for daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(time_series, n_input, n_output):\n",
    "    \"\"\"\n",
    "    Create a dataset with lagged features and future targets for multi-step forecasting.\n",
    "\n",
    "    Args:\n",
    "        time_series (pd.Series): The input time series data.\n",
    "        n_input (int): The number of past time steps to use as input features.\n",
    "        n_output (int): The number of future time steps to predict as targets.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two numpy arrays:\n",
    "               - X: Input features (lagged values).\n",
    "               - y: Target values (future values).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(time_series) - n_input - n_output + 1):\n",
    "        # Extract the input sequence (past n_input values)\n",
    "        input_sequence = time_series.iloc[i:(i + n_input)].values\n",
    "        # Extract the output sequence (next n_output values)\n",
    "        output_sequence = time_series.iloc[(i + n_input):(i + n_input + n_output)].values\n",
    "        X.append(input_sequence)\n",
    "        y.append(output_sequence)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define input and output steps (7 days for both)\n",
    "n_input = 7\n",
    "n_output = 7\n",
    "\n",
    "# Create the dataset using the daily occupied beds series\n",
    "X_daily, y_daily = create_dataset(occupied_beds_daily_series, n_input, n_output)\n",
    "\n",
    "print(f\"Shape of input features (X_daily): {X_daily.shape}\")\n",
    "print(f\"Shape of target values (y_daily): {y_daily.shape}\")\n",
    "\n",
    "# Display the first few rows of X and y\n",
    "print(\"\\nFirst 5 rows of X_daily:\")\n",
    "print(X_daily[:5])\n",
    "\n",
    "print(\"\\nFirst 5 rows of y_daily:\")\n",
    "print(y_daily[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating lagged features, first few rows will have Nan values.\n",
    "\n",
    "So the loop runs for len(time_series) - n_input - n_output + 1\n",
    "1651-7-7+1= 1638\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "Split the prepared daily input (X_daily) and target (y_daily) data into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test set size explicitly to be equivalent to 20 weeks (20 * 7 days)\n",
    "test_set_size = 20 * 7 # 140 days\n",
    "\n",
    "# Determine the split point based on the test set size\n",
    "train_size = len(X_daily) - test_set_size\n",
    "\n",
    "# Split the data\n",
    "X_train = X_daily[:train_size]\n",
    "X_test = X_daily[train_size:]\n",
    "y_train = y_daily[:train_size]\n",
    "y_test = y_daily[train_size:]\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data\n",
    "\n",
    "Apply scaling (e.g., using MinMaxScaler) to the training and testing data. This is particularly important for the LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import min-max\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize and fit scaler for input features (X)\n",
    "scaler_X = MinMaxScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Initialize and fit scaler for target values (y)\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# Print the shapes of the scaled arrays\n",
    "print(f\"Shape of X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"Shape of X_test_scaled: {X_test_scaled.shape}\")\n",
    "print(f\"Shape of y_train_scaled: {y_train_scaled.shape}\")\n",
    "print(f\"Shape of y_test_scaled: {y_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "Scale the training and testing data for both input features and target values using MinMaxScaler, which is particularly important for the LSTM model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape Input for LSTM\n",
    "\n",
    "Reshaping the scaled input data 3D shape for LSTM as it required format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data for LSTM (samples, time steps, features)\n",
    "# Current shape: (samples, n_input) -> Desired shape: (samples, n_input, 1)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Print the new shapes\n",
    "print(f\"Shape of X_train_lstm: {X_train_lstm.shape}\")\n",
    "print(f\"Shape of X_test_lstm: {X_test_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, train, and evaluate Multi-Step Forecasting LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for model building\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "import optuna\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Simplified LSTM model for small datasets\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# Single LSTM layer (no stacking)\n",
    "lstm_model.add(\n",
    "    LSTM(64,  # Reduced units\n",
    "         activation='tanh',\n",
    "         recurrent_dropout=0.1,  # Lower recurrent dropout\n",
    "         kernel_regularizer=l1_l2(l1=0.001, l2=0.001),  # Combined L1/L2 reg\n",
    "         input_shape=(n_input, 1))\n",
    ")\n",
    "\n",
    "# Minimal dense layer (optional - remove if still overfitting)\n",
    "lstm_model.add(Dense(16, activation='tanh'))\n",
    "\n",
    "# Output layer\n",
    "lstm_model.add(Dense(n_output))\n",
    "\n",
    "# Conservative optimizer\n",
    "optimizer = Adam(learning_rate=0.0005)  # Slightly higher LR for small data\n",
    "lstm_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "# 3. Train the LSTM model with EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history_lstm = lstm_model.fit(X_train_lstm, y_train_scaled,\n",
    "                              epochs=200,\n",
    "                              batch_size=64,\n",
    "                              validation_split=0.2,\n",
    "                              callbacks=[early_stop, reduce_lr],\n",
    "                              verbose=1)\n",
    "\n",
    "print(\"LSTM Model Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (loss)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comprehensive Evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, scaler_y, model_name=\"LSTM\"):\n",
    "    \"\"\"\n",
    "    Evaluates model and returns metrics + plots\n",
    "    Returns: dict of metrics, predictions (unscaled)\n",
    "    \"\"\"\n",
    "    # Evaluate on test set\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    if isinstance(test_loss, list):  # Handle case where model returns [loss, mae]\n",
    "        test_loss = test_loss[0]\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_scaled = model.predict(X_test, verbose=0)\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    actuals = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'MSE': mean_squared_error(actuals, predictions),\n",
    "        'RMSE': sqrt(mean_squared_error(actuals, predictions)),\n",
    "        'MAE': mean_absolute_error(actuals, predictions),\n",
    "        'R2': r2_score(actuals, predictions),\n",
    "        'Test_Loss': test_loss\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n--- {model_name} Model Evaluation ---\")\n",
    "    print(f\"Test Loss (scaled): {metrics['Test_Loss']:.4f}\")\n",
    "    print(f\"MSE: {metrics['MSE']:.3f}\")\n",
    "    print(f\"RMSE: {metrics['RMSE']:.3f}\")\n",
    "    print(f\"MAE: {metrics['MAE']:.3f}\")\n",
    "    print(f\"R-squared: {metrics['R2']:.3f}\")\n",
    "\n",
    "    return metrics, predictions\n",
    "\n",
    "# Usage\n",
    "lstm_metrics, lstm_predictions = evaluate_model(\n",
    "    lstm_model,\n",
    "    X_test_lstm,\n",
    "    y_test_scaled,\n",
    "    scaler_y,\n",
    "    \"LSTM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    # Architecture parameters\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "\n",
    "    # Dynamic layer sizing (decaying units pattern)\n",
    "    first_layer_units = trial.suggest_int('first_layer_units', 64, 256, step=32)\n",
    "    layer_decay = trial.suggest_float('layer_decay', 0.5, 0.9)  # How much to reduce units per layer\n",
    "\n",
    "    # Regularization\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.0, 0.3, step=0.1)\n",
    "    l2_reg = trial.suggest_float('l2_reg', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # Training parameters\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    validation_split = trial.suggest_float('validation_split', 0.1, 0.3, step=0.05)\n",
    "    patience = trial.suggest_int('patience', 3, 10)  # For early stopping\n",
    "\n",
    "    # Optimizer configuration\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'nadam'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # ReduceLROnPlateau configuration\n",
    "    reduce_lr_factor = trial.suggest_float('reduce_lr_factor', 0.1, 0.5)\n",
    "    reduce_lr_patience = trial.suggest_int('reduce_lr_patience', 2, 5)\n",
    "\n",
    "    # Model construction\n",
    "    model = Sequential()\n",
    "    units = first_layer_units\n",
    "\n",
    "    # First LSTM layer (always present)\n",
    "    model.add(LSTM(\n",
    "        units,\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        kernel_regularizer=l2(l2_reg),\n",
    "        recurrent_dropout=recurrent_dropout,\n",
    "        return_sequences=(n_layers > 1),\n",
    "        input_shape=(X_train_lstm.shape[1], 1)\n",
    "    ))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Additional LSTM layers\n",
    "    for i in range(1, n_layers):\n",
    "        units = int(units * layer_decay)  # Reduce units for deeper layers\n",
    "        is_last = (i == n_layers - 1)\n",
    "        model.add(LSTM(\n",
    "            units,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            kernel_regularizer=l2(l2_reg),\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            return_sequences=not is_last)\n",
    "        )\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(n_output))\n",
    "\n",
    "    # Optimizer configuration\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = Nadam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=reduce_lr_factor,\n",
    "            patience=reduce_lr_patience,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_lstm,\n",
    "        y_train_scaled,\n",
    "        epochs=100,  # Fixed high value since we use early stopping\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "# 2. Run the study with proper logging\n",
    "def run_optuna_study(n_trials=30):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "\n",
    "    # Add stream handler of stdout to show the messages\n",
    "    optuna.logging.enable_propagation()\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "    study.optimize(create_model, n_trials=n_trials)\n",
    "\n",
    "    # 3. Display full results\n",
    "    print(\"\\n=== Best Trial ===\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Best validation loss: {trial.value:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Best Parameters ===\")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key:>20}: {value}\")\n",
    "\n",
    "    # 4. Visualization\n",
    "    fig1 = plot_optimization_history(study)\n",
    "    fig2 = plot_param_importances(study)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return study\n",
    "\n",
    "# Execute the study\n",
    "study = run_optuna_study(n_trials=30)\n",
    "\n",
    "# 5. Save best parameters to file\n",
    "import json\n",
    "with open('best_params.json', 'w') as f:\n",
    "    json.dump(study.best_params, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assuming X_daily, y_daily, n_input, n_output, scaler_X, and scaler_y are already defined and scaled data is available\n",
    "# If not, you would need to recreate them or ensure they are in the notebook state\n",
    "\n",
    "# For hyperparameter tuning, it's good practice to have a validation set in addition to train and test sets.\n",
    "# Let's split the training data further into training and validation sets.\n",
    "# Using the existing X_train, y_train from the previous split (train_size = len(X_daily) - test_set_size)\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X_train_scaled, y_train_scaled, test_size=0.15, random_state=42 # Using 15% of training data for validation\n",
    ")\n",
    "\n",
    "# Reshape input data for LSTM (samples, time steps, features)\n",
    "X_train_lstm_tune = X_train_tune.reshape((X_train_tune.shape[0], X_train_tune.shape[1], 1))\n",
    "X_val_lstm_tune = X_val_tune.reshape((X_val_tune.shape[0], X_val_tune.shape[1], 1))\n",
    "\n",
    "\n",
    "# Define parameter ranges for tuning\n",
    "lstm_units = [30, 50, 70] # Number of units in the LSTM layer\n",
    "dropout_rates = [0.1, 0.2, 0.3] # Dropout rate\n",
    "batch_sizes = [16, 32, 64] # Batch size\n",
    "epochs_list = [30, 50, 70] # Number of epochs (keep relatively low for grid search speed)\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(lstm_units, dropout_rates, batch_sizes, epochs_list))\n",
    "\n",
    "# Store results\n",
    "results_lstm_tuning = []\n",
    "best_rmse_lstm_tune = float('inf')\n",
    "best_params_lstm_tune = None\n",
    "\n",
    "print(\"Starting parameter tuning for LSTM...\")\n",
    "\n",
    "# Grid search over hyperparameters\n",
    "for units, dropout, batch_size, epochs in tqdm(param_combinations, desc=\"Fitting LSTM models\"):\n",
    "    try:\n",
    "        # Build the LSTM model with current parameters\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units, activation='relu', input_shape=(n_input, 1)))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(n_output))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mse') # Using Adam optimizer\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(X_train_lstm_tune, y_train_tune,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_val_lstm_tune, y_val_tune),\n",
    "                            verbose=0) # Set verbose to 0 to reduce output during tuning\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        loss = model.evaluate(X_val_lstm_tune, y_val_tune, verbose=0)\n",
    "\n",
    "        # Make predictions on the validation set to calculate RMSE\n",
    "        predictions_scaled = model.predict(X_val_lstm_tune, verbose=0)\n",
    "        # Inverse transform to calculate metrics on the original scale\n",
    "        predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "        actuals = scaler_y.inverse_transform(y_val_tune)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "\n",
    "        # Store results\n",
    "        results_lstm_tuning.append(((units, dropout, batch_size, epochs), rmse, loss))\n",
    "\n",
    "        # Check if this is the best model based on RMSE\n",
    "        if rmse < best_rmse_lstm_tune:\n",
    "            best_rmse_lstm_tune = rmse\n",
    "            best_params_lstm_tune = (units, dropout, batch_size, epochs)\n",
    "\n",
    "        print(f\"LSTM(units={units}, dropout={dropout}, batch_size={batch_size}, epochs={epochs}) - Val RMSE: {rmse:.3f}, Val Loss: {loss:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting or evaluating LSTM model for parameters ({units}, {dropout}, {batch_size}, {epochs}): {e}\")\n",
    "        results_lstm_tuning.append(((units, dropout, batch_size, epochs), None, None))\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- Parameter Tuning Complete ---\")\n",
    "print(f\"✅ Best LSTM parameters: {best_params_lstm_tune} with Validation RMSE = {best_rmse_lstm_tune:.3f}\")\n",
    "\n",
    "# Optional: Convert results to DataFrame for easier viewing\n",
    "results_df_lstm_tuning = pd.DataFrame(results_lstm_tuning, columns=['Parameters', 'Val_RMSE', 'Val_Loss'])\n",
    "results_df_lstm_tuning = results_df_lstm_tuning.dropna().sort_values(by='Val_RMSE')\n",
    "print(\"\\nAll Parameter Tuning Results (sorted by Validation RMSE):\")\n",
    "print(results_df_lstm_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Model Architecture\n",
    "\n",
    "We will build a model that uses an Attention mechanism to improve forecasting. This model typically involves:\n",
    "\n",
    "1.  **Encoder**: Processes the input sequence (lagged values).\n",
    "2.  **Attention Mechanism**: Allows the decoder to focus on specific parts of the input sequence.\n",
    "3.  **Decoder**: Generates the output sequence (future values), often using the context from the encoder and the attention mechanism.\n",
    "\n",
    "For this task, we can use a common Encoder-Decoder architecture with Attention, often built using LSTM or GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Dropout, MultiHeadAttention, LayerNormalization, Concatenate, Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "def build_enhanced_model(input_shape=(7, 1), n_output=7):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # 1. Initial Feature Extraction\n",
    "    conv = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv = LayerNormalization()(conv)\n",
    "\n",
    "    # 2. Bidirectional LSTM with residual connection\n",
    "    lstm_out = Bidirectional(\n",
    "        LSTM(128, return_sequences=True,\n",
    "             kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "             recurrent_dropout=0.2)\n",
    "    )(conv)\n",
    "    lstm_out = LayerNormalization()(lstm_out)\n",
    "\n",
    "    # 3. Multi-Head Attention with skip connection\n",
    "    attn_out = MultiHeadAttention(num_heads=4, key_dim=64)(lstm_out, lstm_out)\n",
    "    attn_out = LayerNormalization()(attn_out)\n",
    "    combined = Concatenate(axis=-1)([lstm_out, attn_out])\n",
    "\n",
    "    # 4. Temporal Feature Processing\n",
    "    processed = Bidirectional(LSTM(64, return_sequences=False))(combined)\n",
    "    processed = LayerNormalization()(processed)\n",
    "\n",
    "    # 5. Output with uncertainty estimation\n",
    "    main_output = Dense(n_output, name='main_output')(processed)\n",
    "    aux_output = Dense(n_output, activation='linear', name='aux_output')(processed)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[main_output, aux_output])\n",
    "\n",
    "    # Custom loss weights\n",
    "    losses = {\n",
    "        'main_output': 'mse',\n",
    "        'aux_output': 'mae'\n",
    "    }\n",
    "    loss_weights = {'main_output': 0.8, 'aux_output': 0.2}\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights)\n",
    "    return model\n",
    "\n",
    "# Corrected callbacks with explicit modes\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_main_output_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        mode='min'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_main_output_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_main_output_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize and train\n",
    "model = build_enhanced_model()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_lstm,\n",
    "    {'main_output': y_train_scaled, 'aux_output': y_train_scaled},\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    validation_split=0.15,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['main_output_loss'], label='Train Loss (MSE)')\n",
    "plt.plot(history.history['val_main_output_loss'], label='Validation Loss (MSE)')\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "main_pred, _ = model.predict(X_test_lstm)\n",
    "predictions = scaler_y.inverse_transform(main_pred)\n",
    "actuals = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "mse_direct_attention = mean_squared_error(actuals, predictions)\n",
    "rmse_direct_attention = sqrt(mse_direct_attention)\n",
    "mae_direct_attention = mean_absolute_error(actuals, predictions)\n",
    "r2_direct_attention = r2_score(actuals, predictions)\n",
    "\n",
    "print(\"\\n--- Enhanced Model Evaluation ---\")\n",
    "print(f'MSE: {mse_direct_attention:.3f}')\n",
    "print(f'RMSE: {rmse_direct_attention:.3f}')\n",
    "print(f'MAE: {mae_direct_attention:.3f}')\n",
    "print(f'R²: {r2_direct_attention:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lagged features ( single )\n",
    "\n",
    "This allows us to incorporate the historical patterns and dependencies of the time series into our modeling process, leveraging past values to predict future occupancy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(weekly_series, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many weeks are there in weekly series\n",
    "len(weekly_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features (1 to 4 weeks back)\n",
    "lags = [1, 2, 3, 4]\n",
    "lagged_df = pd.DataFrame({'y': weekly_series})\n",
    "for lag in lags:\n",
    "    lagged_df[f'lag_{lag}'] = weekly_series.shift(lag)\n",
    "\n",
    "# Drop NaN values caused by shifting\n",
    "lagged_df = lagged_df.dropna()\n",
    "\n",
    "# Moving average smoothing (7 and 14 weeks)\n",
    "weekly_df = weekly_series.to_frame(name='y')\n",
    "weekly_df['MA_7'] = weekly_df['y'].rolling(window=7).mean()\n",
    "weekly_df['MA_14'] = weekly_df['y'].rolling(window=14).mean()\n",
    "\n",
    "# Plot moving averages\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(weekly_df['y'], label='Original', alpha=0.6)\n",
    "plt.plot(weekly_df['MA_7'], label='7-Week MA', linestyle='--')\n",
    "plt.plot(weekly_df['MA_14'], label='14-Week MA', linestyle=':')\n",
    "plt.title(\"Weekly Pediatric ICU Occupancy with Moving Averages\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Patients\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "lagged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features and target\n",
    "X = lagged_df.drop(columns=['y']) #lagged features\n",
    "y = lagged_df['y'] # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "test_size = 20\n",
    "train_size = len(X) - test_size\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering ---\n",
    "lagged_df = pd.DataFrame({'y': weekly_series})\n",
    "\n",
    "# 1. Significant lags (from PACF)\n",
    "lagged_df['lag_1'] = weekly_series.shift(1)\n",
    "lagged_df['lag_2'] = weekly_series.shift(2)\n",
    "\n",
    "# 2. Differencing (for non-stationarity)\n",
    "lagged_df['diff_1'] = weekly_series.diff(1).shift(1)  # 1-week difference\n",
    "\n",
    "# 3. Rolling statistics (short-term windows)\n",
    "lagged_df['MA_2'] = weekly_series.rolling(window=2).mean().shift(1)\n",
    "lagged_df['rolling_std_2'] = weekly_series.rolling(window=2).std().shift(1)\n",
    "\n",
    "# Drop rows with NaN (from shifting/rolling)\n",
    "lagged_df = lagged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train/Test Split ---\n",
    "test_size = 20\n",
    "train_size = len(lagged_df) - test_size\n",
    "\n",
    "X = lagged_df.drop(columns=['y'])\n",
    "y = lagged_df['y']\n",
    "\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Scaling ---\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add Gaussian Noise Function ---\n",
    "def add_gaussian_noise(X, mean=0.0, std=0.01):\n",
    "    noise = np.random.normal(loc=mean, scale=std, size=X.shape)\n",
    "    return X + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM Sequence Preparation ---\n",
    "def create_sequences(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Reshape into [samples, time_steps, features]\n",
    "time_steps = 4\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "\n",
    "print(\"Final feature set columns:\", X_train.columns.tolist())\n",
    "print(\"Training shape:\", X_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- LSTM Model Architecture ---\n",
    "model = Sequential([\n",
    "    # First LSTM layer\n",
    "    LSTM(128, activation='relu',\n",
    "         input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "         kernel_regularizer=l2(0.001),\n",
    "         return_sequences=True),\n",
    "    Dropout(0.3),  # Regularization\n",
    "\n",
    "    # Second LSTM layer\n",
    "    LSTM(64, activation='relu',\n",
    "         kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# --- Early Stopping to Prevent Overfitting ---\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=8,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Train the Model ---\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Plot Training History ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Generate and Evaluate Predictions ---\n",
    "y_pred_scaled = model.predict(X_test_seq)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "y_true = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "mse_lstm_single = mean_squared_error(y_true, y_pred)\n",
    "mae_lstm_single = mean_absolute_error(y_true, y_pred)\n",
    "rmse_lstm_single = np.sqrt(mse_lstm_single)\n",
    "r2_lstm_single = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\"\"\n",
    "Evaluation Metrics:\n",
    "- MSE  : {mse_lstm_single:.2f} (Lower is better)\n",
    "- MAE  : {mae_lstm_single:.2f} (Lower is better)\n",
    "- RMSE : {rmse_lstm_single:.2f} (Lower is better)\n",
    "- R²   : {r2_lstm_single:.2f} (1.0 is perfect)\n",
    "\"\"\")\n",
    "\n",
    "# --- Visualize Predictions vs True Values ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_true, label='True Values', color='blue', marker='o', alpha=0.6)\n",
    "plt.plot(y_pred, label='Predictions', color='red', linestyle='--', marker='x')\n",
    "plt.fill_between(\n",
    "    range(len(y_true)),\n",
    "    y_true, y_pred,\n",
    "    color='gray', alpha=0.1, label='Error'\n",
    ")\n",
    "plt.title(f'ICU Bed Occupancy Forecast\\nRMSE: {rmse_lstm_single:.2f}, R²: {r2_lstm_single:.2f}', fontsize=14)\n",
    "plt.xlabel('Week Number', fontsize=12)\n",
    "plt.ylabel('Number of Patients', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUilding the Single-step attention Model\n",
    "def build_single_step_model(input_shape=(1, 4)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial processing\n",
    "    x = Conv1D(filters=32, kernel_size=1, activation='relu')(inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Attention mechanism\n",
    "    attn_out = MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\n",
    "    attn_out = LayerNormalization()(attn_out)\n",
    "\n",
    "    # Combine with original features\n",
    "    combined = Concatenate(axis=-1)([x, attn_out])\n",
    "\n",
    "    # Temporal processing\n",
    "    lstm_out = LSTM(64, return_sequences=False)(combined)\n",
    "    lstm_out = LayerNormalization()(lstm_out)\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, name='main_output')(lstm_out)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "single_step_model = build_single_step_model()\n",
    "single_step_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = single_step_model.fit(\n",
    "    X_train_lstm,\n",
    "    y_train_scaled,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training Vs val loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EValuate\n",
    "# Make predictions\n",
    "y_pred_scaled = single_step_model.predict(X_test_lstm).flatten()\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "mse_direct_single = mean_squared_error(y_test, y_pred)\n",
    "rmse_direct_single = sqrt(mse_direct_single)\n",
    "mae_direct_single = mean_absolute_error(y_test, y_pred)\n",
    "r2_direct_single = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Single-Step Model Evaluation ---\")\n",
    "print(f'MSE: {mse_direct_single:.3f}')\n",
    "print(f'RMSE: {rmse_direct_single:.3f}')\n",
    "print(f'MAE: {mae_direct_single:.3f}')\n",
    "print(f'R²: {r2_direct_single:.3f}')\n",
    "\n",
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual', marker='o')\n",
    "plt.plot(y_test.index, y_pred, label='Predicted', marker='x')\n",
    "plt.title('Single-Step Weekly Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Occupied Beds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using pickle\n",
    "import pickle\n",
    "with open('single_step_model_attention.pkl', 'wb') as file:\n",
    "    pickle.dump(single_step_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define your metric dictionaries\n",
    "multi_step_metrics = {\n",
    "    'MSE': mse_direct_attention,\n",
    "    'RMSE': rmse_direct_attention,\n",
    "    'MAE': mae_direct_attention,\n",
    "    'R2': r2_direct_attention\n",
    "}\n",
    "\n",
    "single_step_metrics = {\n",
    "    'MSE': mse_direct_single,\n",
    "    'RMSE': rmse_direct_single,\n",
    "    'MAE': mae_direct_single,\n",
    "    'R2': r2_direct_single\n",
    "}\n",
    "\n",
    "# Combine into a DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Single-Step': single_step_metrics,\n",
    "    'Multi-Step': multi_step_metrics\n",
    "})\n",
    "\n",
    "# Transpose so each row is a model, each column is a metric\n",
    "metrics_df = metrics_df.T\n",
    "\n",
    "# Create bar plot\n",
    "ax = metrics_df.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "plt.title('Evaluation Metrics: Single-Step vs Multi-Step ATTENTION Direct Forecasts')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(title='Metric')\n",
    "\n",
    "# Add metric values on top of each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', label_type='edge', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative(Rolling forecast Attention+LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create lag features (same as before)\n",
    "lags = [1, 2, 3, 4]\n",
    "lagged_df = pd.DataFrame({'y': weekly_series})\n",
    "for lag in lags:\n",
    "    lagged_df[f'lag_{lag}'] = weekly_series.shift(lag)\n",
    "lagged_df = lagged_df.dropna()\n",
    "\n",
    "# Train-test split (last 20 weeks for testing)\n",
    "test_size = 20\n",
    "train_size = len(lagged_df) - test_size\n",
    "\n",
    "train_data = lagged_df.iloc[:train_size]\n",
    "test_data = lagged_df.iloc[train_size:]\n",
    "\n",
    "# Scale data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(train_data.drop(columns=['y']))\n",
    "y_train = scaler_y.fit_transform(train_data[['y']]).flatten()\n",
    "\n",
    "X_test = scaler_X.transform(test_data.drop(columns=['y']))\n",
    "y_test = scaler_y.transform(test_data[['y']]).flatten()\n",
    "\n",
    "# Reshape for LSTM [samples, timesteps, features]\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, LayerNormalization, MultiHeadAttention, Concatenate\n",
    "\n",
    "def build_iterative_model(input_shape=(1, 4)):  # 4 lag features\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Attention Layer\n",
    "    attn_out = MultiHeadAttention(num_heads=2, key_dim=32)(inputs, inputs)\n",
    "    attn_out = LayerNormalization()(attn_out)\n",
    "\n",
    "    # Combine with original features\n",
    "    combined = Concatenate(axis=-1)([inputs, attn_out])\n",
    "\n",
    "    # LSTM Layer\n",
    "    lstm_out = LSTM(64, return_sequences=False)(combined)\n",
    "    lstm_out = LayerNormalization()(lstm_out)\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1)(lstm_out)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model = build_iterative_model()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_lstm,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def iterative_forecast(model, initial_input, n_steps):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        initial_input: Scaled input vector of shape [1, n_features]\n",
    "        n_steps: Number of steps to predict iteratively\n",
    "    Returns:\n",
    "        List of predicted values (unscaled)\n",
    "    \"\"\"\n",
    "    current_input = initial_input.copy()\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Reshape input for model [1 sample, 1 timestep, n features]\n",
    "        input_reshaped = current_input.reshape(1, 1, -1)\n",
    "\n",
    "        # Predict next value\n",
    "        pred = model.predict(input_reshaped, verbose=0)[0,0]\n",
    "        predictions.append(pred)\n",
    "\n",
    "        # Update input: shift lags and insert prediction\n",
    "        current_input = np.roll(current_input, shift=-1)\n",
    "        current_input[-1] = pred  # Replace last lag with new prediction\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    return scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "# Get the last training input as starting point\n",
    "initial_input = X_train[-1].copy()\n",
    "\n",
    "# Forecast 20 weeks ahead (same length as test set)\n",
    "iterative_preds = iterative_forecast(model, initial_input, n_steps=20)\n",
    "\n",
    "# Compare with actual test values\n",
    "test_values = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(test_values, iterative_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_values, iterative_preds)\n",
    "\n",
    "print(f\"Iterative Forecast Metrics:\")\n",
    "print(f\"MSE: {mse:.2f}, RMSE: {rmse:.2f}, MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import clone_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare initial data (no manual lags needed)\n",
    "train = weekly_series[:-20]\n",
    "test = weekly_series[-20:]\n",
    "\n",
    "# Configuration\n",
    "n_lags = 4  # Number of lagged observations to use as features\n",
    "test_size = len(test)\n",
    "\n",
    "# Initialize storage\n",
    "history = list(train)\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# Create initial lagged dataset\n",
    "def create_lagged_data(series, n_lags):\n",
    "    lagged_data = []\n",
    "    for i in range(n_lags, len(series)):\n",
    "        lagged_data.append(series[i-n_lags:i])\n",
    "    return np.array(lagged_data)\n",
    "\n",
    "# Rolling forecast\n",
    "for t in range(test_size):\n",
    "    # 1. Create lagged features from current history\n",
    "    current_series = np.array(history)\n",
    "    X = create_lagged_data(current_series, n_lags)\n",
    "    y = current_series[n_lags:]\n",
    "\n",
    "    # 2. Scale data\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "    # 3. Clone and retrain model (fresh start each time)\n",
    "    temp_model = clone_model(model)\n",
    "    temp_model.compile(optimizer='adam', loss='mse')\n",
    "    temp_model.fit(X_lstm, y_scaled,\n",
    "                 epochs=50,\n",
    "                 batch_size=16,\n",
    "                 verbose=0)\n",
    "\n",
    "    # 4. Prepare most recent lags for prediction\n",
    "    last_observation = history[-n_lags:]\n",
    "    X_pred = scaler_X.transform(np.array(last_observation).reshape(1, -1))\n",
    "    X_pred_lstm = X_pred.reshape((1, 1, X_pred.shape[1]))\n",
    "\n",
    "    # 5. Make prediction\n",
    "    yhat_scaled = temp_model.predict(X_pred_lstm, verbose=0)[0,0]\n",
    "    yhat = scaler_y.inverse_transform([[yhat_scaled]])[0,0]\n",
    "\n",
    "    # 6. Store results\n",
    "    actual = test[t]\n",
    "    predictions.append(yhat)\n",
    "    actuals.append(actual)\n",
    "    history.append(actual)\n",
    "\n",
    "    print(f'Week {t+1}/{test_size}: Predicted={yhat:.1f}, Actual={actual:.1f}')\n",
    "\n",
    "# Evaluation\n",
    "mse_iterative_attention = mean_squared_error(actuals, predictions)\n",
    "rmse_iterative_attention = sqrt(mse)\n",
    "mae_iterative_attention = mean_absolute_error(actuals, predictions)\n",
    "r2_iterative_attention = r2_score(actuals, predictions)\n",
    "\n",
    "print('\\n--- LSTM Rolling Forecast Evaluation ---')\n",
    "print(f'MSE: {mse_iterative_attention:.1f}')\n",
    "print(f'RMSE: {rmse_iterative_attention:.1f}')\n",
    "print(f'MAE: {mae_iterative_attention:.1f}')\n",
    "print(f'R²: {r2_iterative_attention:.3f}')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(test.index, actuals, label='Actual', marker='o')\n",
    "plt.plot(test.index, predictions, label='LSTM Rolling Forecast', marker='x', linestyle='--')\n",
    "plt.title(' Actual vs Attentiom Rolling Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Occupied Beds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define your metric dictionaries\n",
    "single_step_metrics = {\n",
    "    'MSE': mse_iterative_attention,\n",
    "    'RMSE': rmse_iterative_attention,\n",
    "    'MAE': mae_iterative_attention,\n",
    "    'R2': r2_iterative_attention\n",
    "}\n",
    "\n",
    "multi_step_metrics = {\n",
    "    'MSE': mse_direct_single,\n",
    "    'RMSE': rmse_direct_single,\n",
    "    'MAE': mae_direct_single,\n",
    "    'R2': r2_direct_single\n",
    "}\n",
    "\n",
    "# Combine into a DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Single-Step': single_step_metrics,\n",
    "    'Multi-Step': multi_step_metrics\n",
    "})\n",
    "\n",
    "# Transpose so each row is a model, each column is a metric\n",
    "metrics_df = metrics_df.T\n",
    "\n",
    "# Create bar plot\n",
    "ax = metrics_df.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "plt.title('Evaluation Metrics: Single-Step vs Multi-Step ATTENTION Recursive(Rolling) Forecasts')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.legend(title='Metric')\n",
    "\n",
    "# Add metric values on top of each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', label_type='edge', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "attention_model.save('attention_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Using your actual variables\n",
    "models = ['ARIMA', 'Prophet', 'Attention+LSTM']\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'R²']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Distinct colors for each metric\n",
    "\n",
    "# Organize data by model instead of by metric\n",
    "def organize_by_model(data):\n",
    "    return {\n",
    "        'ARIMA': [data[0][0], data[1][0], data[2][0], data[3][0]],\n",
    "        'Prophet': [data[0][1], data[1][1], data[2][1], data[3][1]],\n",
    "        'Attention+LSTM': [data[0][2], data[1][2], data[2][2], da  ta[3][2]]\n",
    "    }\n",
    "\n",
    "single_step_by_model = organize_by_model(single_step_data)\n",
    "multi_step_by_model = organize_by_model(multi_step_data)\n",
    "\n",
    "# Plot configuration\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(models))\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "def plot_metrics_by_model(ax, data, title):\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [data[model][i] for model in models]\n",
    "        bars = ax.bar(x + i*bar_width, values, width=bar_width,\n",
    "                     color=colors[i], label=metric)\n",
    "\n",
    "        # Add value labels with appropriate precision\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            fmt = '.3f' if metric == 'R²' else '.2f'\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                   f'{height:{fmt}}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x + bar_width*1.5)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    ax.legend()\n",
    "\n",
    "# Generate plots\n",
    "plot_metrics_by_model(ax[0], single_step_by_model, \"Single-Step Forecasting Performance\")\n",
    "plot_metrics_by_model(ax[1], multi_step_by_model, \"Multi-Step Forecasting Performance\")\n",
    "\n",
    "# Final layout\n",
    "fig.suptitle(\"Model Performance Comparison by Metric\", fontsize=14, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
